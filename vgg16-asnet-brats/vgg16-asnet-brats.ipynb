{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3d596b3",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa79aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy keras matplotlib scikit-learn pandas tensorflow[and-cuda] nvidia-cudnn-cu12 h5py -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e27416e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-30 22:51:47.241838: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-30 22:51:47.269882: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743375107.297714  284569 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743375107.305919  284569 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743375107.330328  284569 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743375107.330372  284569 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743375107.330374  284569 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743375107.330376  284569 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-30 22:51:47.339436: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import Model, Input, backend\n",
    "from keras.applications import VGG16\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    BatchNormalization,\n",
    "    MaxPooling2D,\n",
    "    Activation,\n",
    "    UpSampling2D,\n",
    "    concatenate,\n",
    "    Multiply,\n",
    "    GlobalAveragePooling2D,\n",
    "    Dense,\n",
    "    Reshape,\n",
    "    Add, # Import Add layer\n",
    ")\n",
    "from keras.losses import Loss\n",
    "from keras.metrics import Metric\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from tensorflow.keras import mixed_precision\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix # Removed sklearn, use Keras metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f27c06b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit TensorFlow logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # 0 = all, 1 = INFO filtered, 2 = WARNING filtered, 3 = ERROR filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f73243",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "020cdc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider smaller dimensions if memory is tight, but ensure aspect ratio is maintained if possible\n",
    "IMG_HEIGHT = 192\n",
    "IMG_WIDTH = 192  # Changed to square for simplicity, adjust if necessary\n",
    "INPUT_CHANNELS = 3 # For VGG16 input\n",
    "\n",
    "# --- Data Loading Constants ---\n",
    "# Select modalities (Indices based on BraTS documentation: 0:T1, 1:T1ce, 2:T2, 3:FLAIR)\n",
    "MODALITY_INDICES = [1, 3]  # Use T1ce and FLAIR\n",
    "NUM_MODALITIES_LOADED = len(MODALITY_INDICES) # Should be 2\n",
    "# Define which loaded modality corresponds to which RGB channel for VGG16 input\n",
    "# Example: [T1ce, FLAIR, T1ce] -> Index 0, Index 1, Index 0 from the loaded modalities\n",
    "RGB_MAPPING_INDICES = [0, 1, 0] # Map T1ce to R and B, FLAIR to G\n",
    "\n",
    "# --- Training Constants ---\n",
    "BATCH_SIZE = 4      # Keep low due to memory constraints\n",
    "LEARNING_RATE = 1e-4 # Initial learning rate (LOWERED)\n",
    "NUM_EPOCHS = 30     # Number of training epochs (or until early stopping)\n",
    "BUFFER_SIZE = 300   # Shuffle buffer size (adjust based on memory)\n",
    "THRESHOLD = 0.5     # Segmentation threshold for binary metrics and visualization\n",
    "\n",
    "# --- Loss Weights ---\n",
    "# INCREASED CLASS WEIGHT SIGNIFICANTLY\n",
    "COMBINED_LOSS_WEIGHTS = {'bce_weight': 0.5, 'dice_weight': 0.5, 'class_weight': 100.0} # Tunable loss weights\n",
    "# ALTERNATIVE: Use Focal Loss + Dice\n",
    "# USE_FOCAL_LOSS = True # Set to True to use Focal loss instead of WBCE\n",
    "# COMBINED_LOSS_WEIGHTS = {'focal_weight': 0.5, 'dice_weight': 0.5, 'focal_gamma': 2.0}\n",
    "\n",
    "\n",
    "# --- Paths ---\n",
    "# Ensure these paths exist or are created\n",
    "CHECKPOINT_DIR = \"./vgg16-checkpoints\"\n",
    "CHECKPOINT_PATH = f\"{CHECKPOINT_DIR}/vgg16_as_net_model.weights.h5\"\n",
    "CHECKPOINT_BEST_PATH = f\"{CHECKPOINT_DIR}/vgg16_as_net_model_best.weights.h5\"\n",
    "OUTPUT_DIR = \"vgg16-output\"\n",
    "COMPLETION_FILE = \"vgg16-asnet-finished-training.txt\"\n",
    "DATASET_PATH = \"brats2020-training-data/\" # Make sure this points to the directory containing 'BraTS20 Training Metadata.csv' and 'content/data'\n",
    "METADATA_FILE = os.path.join(DATASET_PATH, \"BraTS20 Training Metadata.csv\")\n",
    "H5_DATA_DIR = os.path.join(DATASET_PATH, \"BraTS2020_training_data/content/data\") # Directory containing the .h5 slice files\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"examples\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cda51b",
   "metadata": {},
   "source": [
    "## GPU Configuration and Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "588fa9dd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GPU Configuration ---\n",
      "Physical GPUs: 1, Logical GPUs: 1\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Running on 1 GPU(s) using MirroredStrategy.\n",
      "Global Batch Size (per replica * num replicas): 4\n",
      "\n",
      "--- Mixed Precision Configuration ---\n",
      "Mixed precision policy set to: float32\n",
      "Compute dtype: float32\n",
      "Variable dtype: float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743375111.507003  284569 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6943 MB memory:  -> device: 0, name: GRID V100D-16Q, pci bus id: 0000:00:10.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "print(\"--- GPU Configuration ---\")\n",
    "# Configure memory growth to prevent GPU memory overflow upfront\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "        print(f\"Physical GPUs: {len(gpus)}, Logical GPUs: {len(logical_gpus)}\")\n",
    "\n",
    "        # Optional: Limit GPU memory (uncomment if needed)\n",
    "        # tf.config.set_logical_device_configuration(\n",
    "        #     gpus[0],\n",
    "        #     [tf.config.LogicalDeviceConfiguration(memory_limit=4096)] # Example: 4GB limit\n",
    "        # )\n",
    "        # print(\"Limited GPU memory for logical device.\")\n",
    "\n",
    "        strategy = tf.distribute.MirroredStrategy() # Use if multiple GPUs\n",
    "        print(f\"Running on {strategy.num_replicas_in_sync} GPU(s) using MirroredStrategy.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU Memory Growth Error: {e}. Trying default strategy.\")\n",
    "        strategy = tf.distribute.get_strategy() # Fallback to default\n",
    "        print(\"Running on CPU or single GPU (default strategy).\")\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # Default strategy (CPU or single GPU)\n",
    "    print(\"No GPU detected. Running on CPU.\")\n",
    "print(\"Global Batch Size (per replica * num replicas):\", BATCH_SIZE * strategy.num_replicas_in_sync)\n",
    "\n",
    "print(\"\\n--- Mixed Precision Configuration ---\")\n",
    "# Use mixed precision to reduce memory usage and potentially speed up training on compatible GPUs\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "policy = mixed_precision.Policy('float32') # Using float32 as requested, change if using mixed precision\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print(f\"Mixed precision policy set to: {policy.name}\")\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d033535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure JIT compilation\n",
    "# tf.config.optimizer.set_jit(True) # Can improve performance but might use more memory initially\n",
    "# print(\"JIT compilation enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd5dbda",
   "metadata": {},
   "source": [
    "## Define AS-Net Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b90f328",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def AS_Net(input_size=(IMG_HEIGHT, IMG_WIDTH, INPUT_CHANNELS)):\n",
    "    \"\"\"Defines the AS-Net model with a VGG16 encoder.\"\"\"\n",
    "    inputs = Input(input_size, dtype=tf.float32) # Ensure input dtype is float32\n",
    "\n",
    "    # Load VGG16 backbone pre-trained on ImageNet\n",
    "    # Use compute_dtype from mixed precision policy for the layers\n",
    "    compute_dtype = mixed_precision.global_policy().compute_dtype\n",
    "    VGGnet = VGG16(weights=\"imagenet\", include_top=False, input_tensor=inputs) # Pass input tensor directly\n",
    "\n",
    "    # --- Fine-tuning ---\n",
    "    # Unfreeze layers from block4 onwards (Recommended Start)\n",
    "    for layer in VGGnet.layers:\n",
    "        layer.trainable = False # Freeze initially\n",
    "    # Unfreeze layers from block4 onwards (index > 10 for typical VGG16)\n",
    "    for layer in VGGnet.layers[11:]: # Adjust index based on model.summary() if needed\n",
    "        layer.trainable = True\n",
    "    print(\"Unfroze VGG16 layers from block4 onwards for fine-tuning.\")\n",
    "    # --- End Fine-tuning ---\n",
    "\n",
    "    # Extract feature maps from VGG16 encoder stages\n",
    "    # Layer indices might need adjustment based on VGG16 implementation details. Verify with model.summary().\n",
    "    # Typical VGG16 blocks end at: block1_conv2 (idx 2), block2_conv2 (idx 5), block3_conv3 (idx 9), block4_conv3 (idx 13), block5_conv3 (idx 17)\n",
    "    output1 = VGGnet.get_layer(index=2).output # block1_conv2, Shape: (H, W, 64)\n",
    "    output2 = VGGnet.get_layer(index=5).output # block2_conv2, Shape: (H/2, W/2, 128)\n",
    "    output3 = VGGnet.get_layer(index=9).output # block3_conv3, Shape: (H/4, W/4, 256)\n",
    "    output4 = VGGnet.get_layer(index=13).output # block4_conv3, Shape: (H/8, W/8, 512)\n",
    "    output5 = VGGnet.get_layer(index=17).output # block5_conv3, Shape: (H/16, W/16, 512)\n",
    "\n",
    "    # --- Decoder with SAM, CAM, and Synergy ---\n",
    "    # Upsample block 5, concatenate with block 4\n",
    "    up5 = UpSampling2D((2, 2), interpolation=\"bilinear\", name='up5')(output5)\n",
    "    merge1 = concatenate([output4, up5], axis=-1, name='merge1') # Shape: (H/8, W/8, 512+512=1024)\n",
    "\n",
    "    # Apply SAM and CAM at the first decoder stage\n",
    "    # Input filters = 1024. Output filters = 1024 // 4 = 256\n",
    "    SAM1 = SAM(filters=1024, name='sam1')(merge1)\n",
    "    CAM1 = CAM(filters=1024, name='cam1')(merge1)\n",
    "\n",
    "    # Upsample SAM1/CAM1, concatenate with block 3\n",
    "    up_sam1 = UpSampling2D((2, 2), interpolation=\"bilinear\", name='up_sam1')(SAM1)\n",
    "    up_cam1 = UpSampling2D((2, 2), interpolation=\"bilinear\", name='up_cam1')(CAM1)\n",
    "    merge21 = concatenate([output3, up_sam1], axis=-1, name='merge21') # Shape: (H/4, W/4, 256 + 256 = 512)\n",
    "    merge22 = concatenate([output3, up_cam1], axis=-1, name='merge22') # Shape: (H/4, W/4, 256 + 256 = 512)\n",
    "\n",
    "    # Apply SAM and CAM at the second decoder stage\n",
    "    # Input filters = 512. Output filters = 512 // 4 = 128\n",
    "    SAM2 = SAM(filters=512, name='sam2')(merge21)\n",
    "    CAM2 = CAM(filters=512, name='cam2')(merge22)\n",
    "\n",
    "    # Upsample SAM2/CAM2, concatenate with block 2\n",
    "    up_sam2 = UpSampling2D((2, 2), interpolation=\"bilinear\", name='up_sam2')(SAM2)\n",
    "    up_cam2 = UpSampling2D((2, 2), interpolation=\"bilinear\", name='up_cam2')(CAM2)\n",
    "    merge31 = concatenate([output2, up_sam2], axis=-1, name='merge31') # Shape: (H/2, W/2, 128 + 128 = 256)\n",
    "    merge32 = concatenate([output2, up_cam2], axis=-1, name='merge32') # Shape: (H/2, W/2, 128 + 128 = 256)\n",
    "\n",
    "    # Apply SAM and CAM at the third decoder stage\n",
    "    # Input filters = 256. Output filters = 256 // 4 = 64\n",
    "    SAM3 = SAM(filters=256, name='sam3')(merge31)\n",
    "    CAM3 = CAM(filters=256, name='cam3')(merge32)\n",
    "\n",
    "    # Upsample SAM3/CAM3, concatenate with block 1\n",
    "    up_sam3 = UpSampling2D((2, 2), interpolation=\"bilinear\", name='up_sam3')(SAM3)\n",
    "    up_cam3 = UpSampling2D((2, 2), interpolation=\"bilinear\", name='up_cam3')(CAM3)\n",
    "    merge41 = concatenate([output1, up_sam3], axis=-1, name='merge41') # Shape: (H, W, 64 + 64 = 128)\n",
    "    merge42 = concatenate([output1, up_cam3], axis=-1, name='merge42') # Shape: (H, W, 64 + 64 = 128)\n",
    "\n",
    "    # Apply SAM and CAM at the fourth decoder stage\n",
    "    # Input filters = 128. Output filters = 128 // 4 = 32\n",
    "    SAM4 = SAM(filters=128, name='sam4')(merge41)\n",
    "    CAM4 = CAM(filters=128, name='cam4')(merge42)\n",
    "\n",
    "    # Synergy module to combine final SAM and CAM outputs\n",
    "    synergy_output = Synergy(name='synergy')((SAM4, CAM4)) # Input shapes: (H, W, 32) + (H, W, 32) -> Synergy Conv(filters=1) -> (H, W, 1)\n",
    "\n",
    "    # Final 1x1 convolution for segmentation map\n",
    "    # Use float32 for the final output layer for numerical stability, even in mixed precision\n",
    "    output = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\", name='final_output', dtype='float32')(synergy_output)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=inputs, outputs=output, name='AS_Net_VGG16')\n",
    "\n",
    "    # Clean up memory after building model (optional)\n",
    "    gc.collect()\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- SAM Module ---\n",
    "class SAM(Model):\n",
    "    \"\"\"Spatial Attention Module\"\"\"\n",
    "    def __init__(self, filters, name='sam', **kwargs):\n",
    "        super(SAM, self).__init__(name=name, **kwargs)\n",
    "        self.filters = filters\n",
    "        self.out_channels = filters // 4 # Output channels for SAM/CAM in AS-Net paper\n",
    "\n",
    "        # Convolution layers using compute dtype\n",
    "        compute_dtype = mixed_precision.global_policy().compute_dtype\n",
    "        self.conv1 = Conv2D(self.out_channels, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv1')\n",
    "        self.conv2 = Conv2D(self.out_channels, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv2')\n",
    "        self.conv3 = Conv2D(self.out_channels, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv3')\n",
    "        self.conv4 = Conv2D(self.out_channels, 1, activation=\"relu\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv4') # Branch for attention weights (shortcut path)\n",
    "\n",
    "        # Pooling and Upsampling\n",
    "        self.pool1 = MaxPooling2D((2, 2), name='pool1')\n",
    "        self.upsample1 = UpSampling2D((2, 2), interpolation=\"bilinear\", name='upsample1')\n",
    "        self.W1 = Conv2D(self.out_channels, 1, activation=\"sigmoid\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='W1')\n",
    "\n",
    "        self.pool2 = MaxPooling2D((4, 4), name='pool2')\n",
    "        self.upsample2 = UpSampling2D((4, 4), interpolation=\"bilinear\", name='upsample2')\n",
    "        self.W2 = Conv2D(self.out_channels, 1, activation=\"sigmoid\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='W2')\n",
    "\n",
    "        self.activation = Activation('relu', name='relu_act')\n",
    "        self.multiply = Multiply(name='multiply')\n",
    "        self.add = Add(name='add') # Use Add layer\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out1 = self.conv3(self.conv2(self.conv1(inputs))) # Main feature path F'(X)\n",
    "        out2 = self.conv4(inputs) # Shortcut path F''(X)\n",
    "\n",
    "        # Parallel attention branches (on shortcut path F''(X))\n",
    "        pooled1 = self.pool1(out2)\n",
    "        upsampled1 = self.upsample1(pooled1)\n",
    "        activated1 = self.activation(upsampled1) # Apply activation *before* W1\n",
    "        merge1 = self.W1(activated1) # S1\n",
    "\n",
    "        pooled2 = self.pool2(out2)\n",
    "        upsampled2 = self.upsample2(pooled2)\n",
    "        activated2 = self.activation(upsampled2) # Apply activation *before* W2\n",
    "        merge2 = self.W2(activated2) # S2\n",
    "\n",
    "        # Combine attention weights (S)\n",
    "        out3 = self.add([merge1, merge2]) # Element-wise addition S = S1 + S2\n",
    "\n",
    "        # Apply attention: Multiply main path by attention weights and add shortcut\n",
    "        # Y = F'(X) * S + F''(X)\n",
    "        y = self.multiply([out1, out3]) # F'(X) * S\n",
    "        y = self.add([y, out2]) # Add shortcut connection F''(X)\n",
    "        return y\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SAM, self).get_config()\n",
    "        config.update({\"filters\": self.filters})\n",
    "        return config\n",
    "\n",
    "# --- CAM Module ---\n",
    "class CAM(Model):\n",
    "    \"\"\"Channel Attention Module\"\"\"\n",
    "    def __init__(self, filters, reduction_ratio=16, name='cam', **kwargs):\n",
    "        super(CAM, self).__init__(name=name, **kwargs)\n",
    "        self.filters = filters\n",
    "        self.out_channels = filters // 4 # Output channels for SAM/CAM in AS-Net paper\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "\n",
    "        # Convolution layers using compute dtype\n",
    "        compute_dtype = mixed_precision.global_policy().compute_dtype\n",
    "        self.conv1 = Conv2D(self.out_channels, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv1')\n",
    "        self.conv2 = Conv2D(self.out_channels, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv2')\n",
    "        self.conv3 = Conv2D(self.out_channels, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv3')\n",
    "        self.conv4 = Conv2D(self.out_channels, 1, activation=\"relu\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv4') # Branch for attention weights (shortcut path)\n",
    "\n",
    "        # Channel attention mechanism\n",
    "        self.gpool = GlobalAveragePooling2D(name='global_avg_pool')\n",
    "        # Dense layers for channel attention weights (use compute dtype)\n",
    "        self.fc1 = Dense(self.out_channels // self.reduction_ratio, activation=\"relu\", use_bias=False, dtype=compute_dtype, name='fc1')\n",
    "        self.fc2 = Dense(self.out_channels, activation=\"sigmoid\", use_bias=False, dtype=compute_dtype, name='fc2')\n",
    "        # Reshape needed to multiply with conv output (Batch, H, W, C) -> (Batch, 1, 1, C)\n",
    "        self.reshape = Reshape((1, 1, self.out_channels), name='reshape')\n",
    "\n",
    "        self.multiply = Multiply(name='multiply')\n",
    "        self.add = Add(name='add') # Use Add layer\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out1 = self.conv3(self.conv2(self.conv1(inputs))) # Main feature path F'(X)\n",
    "        out2 = self.conv4(inputs) # Shortcut path F''(X)\n",
    "\n",
    "        # Calculate channel attention weights (C) from shortcut path F''(X)\n",
    "        pooled = self.gpool(out2) # Global Average Pooling -> (Batch, C_out)\n",
    "        fc1_out = self.fc1(pooled)\n",
    "        fc2_out = self.fc2(fc1_out)\n",
    "        out3 = self.reshape(fc2_out) # Reshape to (Batch, 1, 1, C_out) for broadcasting\n",
    "\n",
    "        # Apply attention: Multiply main path by channel attention weights and add shortcut\n",
    "        # Y = F'(X) * C + F''(X)\n",
    "        y = self.multiply([out1, out3]) # Broadcasting applies channel weights F'(X) * C\n",
    "        y = self.add([y, out2]) # Add shortcut connection F''(X)\n",
    "        return y\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CAM, self).get_config()\n",
    "        config.update({\"filters\": self.filters, \"reduction_ratio\": self.reduction_ratio})\n",
    "        return config\n",
    "\n",
    "# --- Synergy Module ---\n",
    "class Synergy(Model):\n",
    "    \"\"\"Combines SAM and CAM outputs with learnable weights.\"\"\"\n",
    "    def __init__(self, alpha_init=0.5, beta_init=0.5, name='synergy', **kwargs):\n",
    "        super(Synergy, self).__init__(name=name, **kwargs)\n",
    "        # Use tf.Variable for learnable weights\n",
    "        self.alpha = tf.Variable(alpha_init, trainable=True, name=\"alpha\", dtype=tf.float32)\n",
    "        self.beta = tf.Variable(beta_init, trainable=True, name=\"beta\", dtype=tf.float32)\n",
    "\n",
    "        # 1x1 Convolution after weighted sum, followed by BN\n",
    "        compute_dtype = mixed_precision.global_policy().compute_dtype\n",
    "        # Output should be 1 channel before the final sigmoid in AS_Net\n",
    "        self.conv = Conv2D(1, 1, padding=\"same\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv')\n",
    "        self.bn = BatchNormalization(name='bn')\n",
    "        self.add = Add(name='add_weighted') # Use Add layer\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, y = inputs # Expecting a tuple/list of (SAM_output, CAM_output)\n",
    "\n",
    "        # Cast learnable weights to the compute dtype for multiplication\n",
    "        compute_dtype = x.dtype # Get dtype from input tensor\n",
    "        alpha_casted = tf.cast(self.alpha, compute_dtype)\n",
    "        beta_casted = tf.cast(self.beta, compute_dtype)\n",
    "\n",
    "        # Weighted sum: alpha * SAM_output + beta * CAM_output\n",
    "        weighted_sum = self.add([alpha_casted * x, beta_casted * y])\n",
    "\n",
    "        # Apply Conv -> BN\n",
    "        convolved = self.conv(weighted_sum)\n",
    "        bn_out = self.bn(convolved)\n",
    "        # No activation here, final sigmoid is in the main model output layer\n",
    "        return bn_out # Output has 1 channel\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Synergy, self).get_config()\n",
    "        # Store initial values, actual values are saved in weights\n",
    "        config.update({\"alpha_init\": 0.5, \"beta_init\": 0.5})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4fc192",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8bd5b4c",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DiceLoss(Loss):\n",
    "    \"\"\"Computes the Dice Loss, a common metric for segmentation tasks.\"\"\"\n",
    "    def __init__(self, smooth=1e-6, name='dice_loss', **kwargs):\n",
    "        # super(DiceLoss, self).__init__(name=name, reduction=tf.keras.losses.Reduction.AUTO, **kwargs) # OLD\n",
    "        super(DiceLoss, self).__init__(name=name, reduction='sum_over_batch_size', **kwargs) # NEW\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, y_pred.dtype) # Ensure same dtype\n",
    "\n",
    "        # Flatten tensors to calculate Dice score using tf.reshape\n",
    "        y_true_f = tf.reshape(y_true, [-1])\n",
    "        y_pred_f = tf.reshape(y_pred, [-1])\n",
    "\n",
    "        # Calculate intersection and union using tf.reduce_sum instead of backend.sum\n",
    "        intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "        union = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f)\n",
    "\n",
    "        # Calculate Dice coefficient\n",
    "        dice_coef = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        # Return Dice Loss\n",
    "        return 1.0 - dice_coef\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DiceLoss, self).get_config()\n",
    "        config.update({\"smooth\": self.smooth})\n",
    "        return config\n",
    "\n",
    "\n",
    "class WBCE(Loss):\n",
    "    \"\"\"Weighted Binary Cross-Entropy Loss.\"\"\"\n",
    "    def __init__(self, weight=1.0, name='weighted_bce_loss', **kwargs):\n",
    "        # super(WBCE, self).__init__(name=name, reduction=tf.keras.losses.Reduction.AUTO, **kwargs) # OLD\n",
    "        super(WBCE, self).__init__(name=name, reduction='sum_over_batch_size', **kwargs) # NEW\n",
    "        self.weight = tf.cast(weight, tf.float32)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "\n",
    "        # Clip predictions to avoid log(0)\n",
    "        epsilon_ = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon_, 1.0 - epsilon_)\n",
    "\n",
    "        # Calculate weighted BCE using tf.nn.weighted_cross_entropy_with_logits\n",
    "        # Since model output is sigmoid, convert back to logits for the TF function\n",
    "        logits = tf.math.log(y_pred / (1.0 - y_pred))\n",
    "\n",
    "        # TF function expects targets (y_true) and logits\n",
    "        loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "            labels=y_true,\n",
    "            logits=logits,\n",
    "            pos_weight=self.weight # This is where the weight is applied\n",
    "        )\n",
    "\n",
    "        # The function returns loss per element, so reduce it (e.g., mean)\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(WBCE, self).get_config()\n",
    "        config.update({\"weight\": self.weight.numpy()}) # Store numpy value in config\n",
    "        return config\n",
    "\n",
    "# Alternative Loss Class using Focal Loss:\n",
    "class CombinedFocalDiceLoss(Loss):\n",
    "    \"\"\"Combines Dice Loss and Binary Focal Crossentropy.\"\"\"\n",
    "    def __init__(self, focal_weight=0.5, dice_weight=0.5, focal_gamma=2.0, focal_alpha=None, name='combined_focal_dice_loss', **kwargs):\n",
    "        # super().__init__(name=name, reduction=tf.keras.losses.Reduction.AUTO, **kwargs) # OLD\n",
    "        super().__init__(name=name, reduction='sum_over_batch_size', **kwargs)\n",
    "        self.focal_weight = focal_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        # Use alpha for class weighting in focal loss if needed (e.g., 0.25 for positive class)\n",
    "        # apply_class_balancing=True can be problematic, manual alpha is often better\n",
    "        self.focal_loss = tf.keras.losses.BinaryFocalCrossentropy(\n",
    "            #apply_class_balancing=True, # May not be stable, use alpha instead if needed\n",
    "            alpha=focal_alpha, # e.g., 0.25 or calculate based on imbalance\n",
    "            gamma=focal_gamma,\n",
    "            from_logits=False # Model output is sigmoid\n",
    "        )\n",
    "        self.dice_loss = DiceLoss()\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        focal_loss_val = self.focal_loss(y_true, y_pred)\n",
    "        dice_loss_val = self.dice_loss(y_true, y_pred)\n",
    "        combined = (self.focal_weight * focal_loss_val) + (self.dice_weight * dice_loss_val)\n",
    "        return combined\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CombinedFocalDiceLoss, self).get_config()\n",
    "        config.update({\n",
    "            \"focal_weight\": self.focal_weight,\n",
    "            \"dice_weight\": self.dice_weight,\n",
    "            \"focal_gamma\": self.focal_loss.gamma,\n",
    "            \"focal_alpha\": self.focal_loss.alpha\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Combined Loss (Example: Dice + Weighted BCE)\n",
    "class CombinedLoss(Loss):\n",
    "    \"\"\"Combines Dice Loss and Weighted Binary Cross-Entropy.\"\"\"\n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5, class_weight=1.0, name='combined_loss', **kwargs):\n",
    "        # super(CombinedLoss, self).__init__(name=name, reduction=tf.keras.losses.Reduction.AUTO, **kwargs) # OLD\n",
    "        super(CombinedLoss, self).__init__(name=name, reduction='sum_over_batch_size', **kwargs) # NEW\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.wbce = WBCE(weight=class_weight)\n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.wbce = WBCE(weight=class_weight) # Instantiate WBCE with class weight\n",
    "        self.dice_loss = DiceLoss()          # Instantiate Dice Loss\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        # Calculate individual losses\n",
    "        bce_loss_val = self.wbce(y_true, y_pred)\n",
    "        dice_loss_val = self.dice_loss(y_true, y_pred)\n",
    "\n",
    "        # Combine losses with weights\n",
    "        combined = (self.bce_weight * bce_loss_val) + (self.dice_weight * dice_loss_val)\n",
    "        return combined\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CombinedLoss, self).get_config()\n",
    "        config.update({\n",
    "            \"bce_weight\": self.bce_weight,\n",
    "            \"dice_weight\": self.dice_weight,\n",
    "            \"class_weight\": self.wbce.weight.numpy() # Get class weight from WBCE instance\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847dccc5",
   "metadata": {},
   "source": [
    "## Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c235ad2",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Dice Coefficient Metric\n",
    "class DiceCoefficient(Metric):\n",
    "    \"\"\"Computes the Dice Coefficient metric.\"\"\"\n",
    "    def __init__(self, threshold=THRESHOLD, smooth=1e-6, name='dice_coefficient', dtype=None):\n",
    "        super(DiceCoefficient, self).__init__(name=name, dtype=dtype)\n",
    "        self.threshold = threshold\n",
    "        self.smooth = smooth\n",
    "        # Use state variables to accumulate counts over batches\n",
    "        self.intersection_sum = self.add_weight(name='intersection_sum', initializer='zeros', dtype=tf.float32)\n",
    "        self.union_sum = self.add_weight(name='union_sum', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32) # Use float32 for internal calculations\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "        # Apply threshold to predictions\n",
    "        y_pred_binary = tf.cast(y_pred >= self.threshold, tf.float32)\n",
    "\n",
    "        # Flatten\n",
    "        y_true_f = tf.reshape(y_true, [-1])\n",
    "        y_pred_f = tf.reshape(y_pred_binary, [-1])\n",
    "\n",
    "        # Calculate intersection and sum for the current batch\n",
    "        intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "        pred_sum = tf.reduce_sum(y_pred_f)\n",
    "        true_sum = tf.reduce_sum(y_true_f)\n",
    "\n",
    "        # Update state variables\n",
    "        self.intersection_sum.assign_add(intersection)\n",
    "        self.union_sum.assign_add(true_sum + pred_sum)\n",
    "\n",
    "    def result(self):\n",
    "        # Calculate Dice coefficient from accumulated sums\n",
    "        dice = (2.0 * self.intersection_sum + self.smooth) / (self.union_sum + self.smooth)\n",
    "        # Return result potentially casted to the metric's dtype\n",
    "        return tf.cast(dice, self._dtype) if self._dtype else dice\n",
    "\n",
    "\n",
    "    def reset_state(self):\n",
    "        # Keras docs recommend using assign(0.)\n",
    "        self.intersection_sum.assign(0.0)\n",
    "        self.union_sum.assign(0.0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DiceCoefficient, self).get_config()\n",
    "        config.update({\"threshold\": self.threshold, \"smooth\": self.smooth})\n",
    "        return config\n",
    "\n",
    "\n",
    "# IoU Metric (Jaccard)\n",
    "class IoU(Metric):\n",
    "    \"\"\"Computes the Intersection over Union (IoU) or Jaccard Index.\"\"\"\n",
    "    def __init__(self, threshold=THRESHOLD, smooth=1e-6, name='iou', dtype=None):\n",
    "        super(IoU, self).__init__(name=name, dtype=dtype)\n",
    "        self.threshold = threshold\n",
    "        self.smooth = smooth\n",
    "        self.intersection_sum = self.add_weight(name='intersection_sum', initializer='zeros', dtype=tf.float32)\n",
    "        self.union_sum = self.add_weight(name='union_sum', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred_binary = tf.cast(y_pred >= self.threshold, tf.float32)\n",
    "\n",
    "        y_true_f = tf.reshape(y_true, [-1])\n",
    "        y_pred_f = tf.reshape(y_pred_binary, [-1])\n",
    "\n",
    "        intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "        true_sum = tf.reduce_sum(y_true_f)\n",
    "        pred_sum = tf.reduce_sum(y_pred_f)\n",
    "\n",
    "        # Union = Sum(A) + Sum(B) - Intersection(A,B)\n",
    "        union = true_sum + pred_sum - intersection\n",
    "\n",
    "        self.intersection_sum.assign_add(intersection)\n",
    "        self.union_sum.assign_add(union)\n",
    "\n",
    "    def result(self):\n",
    "        iou = (self.intersection_sum + self.smooth) / (self.union_sum + self.smooth)\n",
    "        return tf.cast(iou, self._dtype) if self._dtype else iou\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.intersection_sum.assign(0.0)\n",
    "        self.union_sum.assign(0.0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(IoU, self).get_config()\n",
    "        config.update({\"threshold\": self.threshold, \"smooth\": self.smooth})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21858f1a",
   "metadata": {},
   "source": [
    "## Data Preparation for BraTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cd7c7f7",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_brats_data_gpu(\n",
    "    metadata_file=METADATA_FILE,\n",
    "    h5_dir=H5_DATA_DIR,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    modality_indices=MODALITY_INDICES, # Indices to load\n",
    "    rgb_mapping_indices=RGB_MAPPING_INDICES, # How loaded modalities map to RGB\n",
    "    num_modalities_loaded=NUM_MODALITIES_LOADED,\n",
    "    input_channels=INPUT_CHANNELS, # Should be 3 for VGG16\n",
    "    validation_split=0.2,\n",
    "    random_seed=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepares the BraTS 2020 dataset from H5 slices using tf.data.\n",
    "    Optimized for memory and GPU processing. Assumes H5 files contain slices.\n",
    "    \"\"\"\n",
    "    print(\"--- Setting up Data Pipeline ---\")\n",
    "    print(f\"Loading metadata from: {metadata_file}\")\n",
    "    print(f\"Loading H5 data from: {h5_dir}\")\n",
    "    print(f\"Target image size: {target_size}\")\n",
    "    print(f\"Modalities loaded (indices): {modality_indices}\")\n",
    "    print(f\"Modalities mapped to RGB (indices): {rgb_mapping_indices}\")\n",
    "\n",
    "    if not os.path.exists(metadata_file):\n",
    "        raise FileNotFoundError(f\"Metadata file not found at {metadata_file}\")\n",
    "    if not os.path.exists(h5_dir):\n",
    "        raise FileNotFoundError(f\"H5 data directory not found at {h5_dir}\")\n",
    "\n",
    "    # Read metadata and filter for existing files\n",
    "    df = pd.read_csv(metadata_file)\n",
    "\n",
    "    # Construct full path and check existence\n",
    "    df['full_path'] = df['slice_path'].apply(lambda x: os.path.join(h5_dir, os.path.basename(x)))\n",
    "    df = df[df['full_path'].apply(os.path.exists)].copy() # Filter out rows where the H5 file doesn't exist\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No valid H5 files found based on metadata in {h5_dir}. Check paths.\")\n",
    "\n",
    "    print(f\"Found {len(df)} existing H5 files referenced in metadata.\")\n",
    "\n",
    "    # Shuffle and split data\n",
    "    df = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    split_idx = int(len(df) * (1.0 - validation_split))\n",
    "    train_df = df.iloc[:split_idx]\n",
    "    val_df = df.iloc[split_idx:]\n",
    "    print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
    "\n",
    "    # Get lists of file paths\n",
    "    train_files = train_df[\"full_path\"].tolist()\n",
    "    val_files = val_df[\"full_path\"].tolist()\n",
    "\n",
    "    # Function to parse a single H5 file using tf.py_function\n",
    "    def parse_h5_file(file_path):\n",
    "        def _parse_h5(path_tensor):\n",
    "            path = path_tensor.numpy().decode(\"utf-8\")\n",
    "            try:\n",
    "                with h5py.File(path, \"r\") as hf:\n",
    "                    image_data = hf[\"image\"][()] # Shape (H, W, 4) - float64\n",
    "                    mask_data = hf[\"mask\"][()]   # Shape (H, W, 3) - uint8\n",
    "\n",
    "                    # --- Select Modalities ---\n",
    "                    # Select the specified modalities (e.g., T1ce, FLAIR)\n",
    "                    selected_modalities = image_data[:, :, modality_indices].astype(np.float32) # Shape (H, W, num_modalities_loaded)\n",
    "\n",
    "                    # --- Create Binary Mask ---\n",
    "                    # Combine all tumor classes (NCR/NET=1, ED=2, ET=4) into a single binary mask\n",
    "                    # Assumes mask channels are [NCR/NET, ED, ET]\n",
    "                    # Check mask channel interpretation if different\n",
    "                    binary_mask = np.logical_or.reduce((mask_data[:, :, 0] > 0,\n",
    "                                                        mask_data[:, :, 1] > 0,\n",
    "                                                        mask_data[:, :, 2] > 0)).astype(np.float32) # Shape (H, W)\n",
    "\n",
    "                    # Debug: Print shapes and types after loading\n",
    "                    # print(f\"Loaded {os.path.basename(path)}: Image shape {selected_modalities.shape}, Mask shape {binary_mask.shape}\")\n",
    "                    # Check mask values\n",
    "                    # unique_mask_vals = np.unique(binary_mask)\n",
    "                    # if not np.all(np.isin(unique_mask_vals, [0, 1])):\n",
    "                    #     print(f\"WARNING: Non-binary mask values found in {os.path.basename(path)}: {unique_mask_vals}\")\n",
    "\n",
    "                    return selected_modalities, binary_mask\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {path}: {e}\")\n",
    "                # Return dummy data of expected original shape if loading fails\n",
    "                original_h, original_w = 240, 240 # Assuming original size\n",
    "                return (\n",
    "                    np.zeros((original_h, original_w, num_modalities_loaded), dtype=np.float32),\n",
    "                    np.zeros((original_h, original_w), dtype=np.float32),\n",
    "                )\n",
    "\n",
    "        # Wrap the Python function\n",
    "        image, mask = tf.py_function(\n",
    "            _parse_h5, [file_path], [tf.float32, tf.float32]\n",
    "        )\n",
    "\n",
    "        # --- Set Shapes ---\n",
    "        # Explicitly set shapes after py_function, crucial for tf.data pipeline\n",
    "        original_h, original_w = 240, 240 # Set expected original dimensions\n",
    "        image.set_shape([original_h, original_w, num_modalities_loaded])\n",
    "        mask.set_shape([original_h, original_w])\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    # Function to preprocess image and mask tensors\n",
    "    def preprocess(image, mask):\n",
    "        # --- Normalization ---\n",
    "        # Apply Z-score normalization per channel (modality)\n",
    "        normalized_channels = []\n",
    "        for i in range(num_modalities_loaded):\n",
    "            channel = image[:, :, i]\n",
    "            mean = tf.reduce_mean(channel)\n",
    "            std = tf.math.reduce_std(channel)\n",
    "            # Add epsilon to prevent division by zero if a channel is constant\n",
    "            normalized_channel = (channel - mean) / (std + 1e-8)\n",
    "            normalized_channels.append(normalized_channel)\n",
    "        image_normalized = tf.stack(normalized_channels, axis=-1) # Shape: (H, W, num_modalities_loaded)\n",
    "\n",
    "        # --- RGB Mapping ---\n",
    "        # Create 3-channel image for VGG16 by mapping/duplicating normalized modalities\n",
    "        # Example: T1ce -> R, FLAIR -> G, T1ce -> B\n",
    "        rgb_channels = [image_normalized[:, :, idx] for idx in rgb_mapping_indices]\n",
    "        image_rgb = tf.stack(rgb_channels, axis=-1) # Shape: (H, W, 3)\n",
    "\n",
    "        # --- Resizing ---\n",
    "        # Resize image using bilinear interpolation\n",
    "        image_resized = tf.image.resize(image_rgb, target_size, method='bilinear')\n",
    "        # Resize mask using nearest neighbor to preserve binary values\n",
    "        # Add channel dim to mask for resize, then remove\n",
    "        mask_expanded = tf.expand_dims(mask, axis=-1)\n",
    "        mask_resized = tf.image.resize(mask_expanded, target_size, method='nearest')\n",
    "        mask_resized = tf.squeeze(mask_resized, axis=-1)\n",
    "\n",
    "        # --- Final Mask Processing ---\n",
    "        # Ensure mask is binary (0 or 1) after resizing\n",
    "        mask_final = tf.cast(mask_resized > 0.5, tf.float32)\n",
    "        # Add channel dimension to mask for consistency with model output shape (H, W, 1)\n",
    "        mask_final = tf.expand_dims(mask_final, axis=-1)\n",
    "\n",
    "        # --- Set Final Shapes ---\n",
    "        image_resized.set_shape([target_size[0], target_size[1], input_channels])\n",
    "        mask_final.set_shape([target_size[0], target_size[1], 1])\n",
    "\n",
    "        # Debug: Print shapes and types during preprocessing\n",
    "        # tf.print(\"Preprocessed Image Shape:\", tf.shape(image_resized), \"Mask Shape:\", tf.shape(mask_final))\n",
    "        # tf.print(\"Image dtype:\", image_resized.dtype, \"Mask dtype:\", mask_final.dtype)\n",
    "        # tf.print(\"Unique mask values after preprocess:\", tf.unique(tf.reshape(mask_final, [-1]))[0])\n",
    "\n",
    "        return image_resized, mask_final\n",
    "\n",
    "    # --- Create tf.data Datasets ---\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(val_files)\n",
    "\n",
    "    # --- Apply Transformations ---\n",
    "    # Use AUTOTUNE for parallel calls\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA # Or FILE\n",
    "\n",
    "    train_dataset = (\n",
    "        train_dataset.with_options(options)\n",
    "        .map(parse_h5_file, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .shuffle(buffer_size) # Shuffle after loading and preprocessing\n",
    "        .batch(batch_size)    # Batch data\n",
    "        .prefetch(tf.data.AUTOTUNE) # Prefetch for performance\n",
    "    )\n",
    "    val_dataset = (\n",
    "        val_dataset.with_options(options)\n",
    "        .map(parse_h5_file, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    print(\"Data pipeline created successfully.\")\n",
    "    # Optional: Inspect element spec\n",
    "    # print(\"Train Dataset Element Spec:\", train_dataset.element_spec)\n",
    "    # print(\"Validation Dataset Element Spec:\", val_dataset.element_spec)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# Function to visualize samples from the dataset\n",
    "def visualize_dataset_samples(dataset, num_samples=3, output_dir=OUTPUT_DIR):\n",
    "    \"\"\"Visualizes samples from the dataset and saves the plot.\"\"\"\n",
    "    print(\"--- Visualizing Dataset Samples ---\")\n",
    "    try:\n",
    "        plt.figure(figsize=(15, 5 * num_samples))\n",
    "        plot_count = 0\n",
    "        for images, masks in dataset.take(1): # Take one batch\n",
    "            num_in_batch = images.shape[0]\n",
    "            print(f\"Batch shapes - Images: {images.shape}, Masks: {masks.shape}\")\n",
    "            print(f\"Image dtype: {images.dtype}, Mask dtype: {masks.dtype}\")\n",
    "            print(f\"Image value range: Min={tf.reduce_min(images):.4f}, Max={tf.reduce_max(images):.4f}\")\n",
    "            print(f\"Mask value range: Min={tf.reduce_min(masks):.4f}, Max={tf.reduce_max(masks):.4f}\")\n",
    "            print(f\"Unique mask values in batch: {tf.unique(tf.reshape(masks, [-1]))[0]}\")\n",
    "\n",
    "\n",
    "            for i in range(min(num_samples, num_in_batch)):\n",
    "                plot_count += 1\n",
    "                img = images[i].numpy()\n",
    "                mask = masks[i].numpy().squeeze() # Remove channel dim for plotting\n",
    "\n",
    "                # --- Display individual channels and combined RGB ---\n",
    "                # Assuming RGB mapping [T1ce, FLAIR, T1ce] -> Channels 0, 1, 2\n",
    "                titles = [\"Input R (T1ce)\", \"Input G (FLAIR)\", \"Input B (T1ce)\", \"RGB Input\", \"Ground Truth Mask\"]\n",
    "                channels_to_plot = [img[:, :, 0], img[:, :, 1], img[:, :, 2], img, mask]\n",
    "\n",
    "                for j, item in enumerate(channels_to_plot):\n",
    "                    ax = plt.subplot(num_samples, len(titles), i * len(titles) + j + 1)\n",
    "                    ax.set_title(titles[j])\n",
    "\n",
    "                    # Normalize individual channels for display if needed (using percentiles)\n",
    "                    if j < 3: # Individual channels\n",
    "                         p_low, p_high = np.percentile(item, [2, 98])\n",
    "                         item_disp = np.clip((item - p_low) / (p_high - p_low + 1e-8), 0, 1) if (p_high - p_low) > 1e-7 else item\n",
    "                         plt.imshow(item_disp, cmap='gray')\n",
    "                    elif j == 3: # RGB image\n",
    "                        # RGB image might already be normalized, display as is or clip\n",
    "                        img_rgb_disp = np.clip(item, 0, 1) # Clip Z-score normalized data to [0,1] for display\n",
    "                        # Check for NaN/Inf\n",
    "                        if np.isnan(img_rgb_disp).any() or np.isinf(img_rgb_disp).any():\n",
    "                             print(f\"Warning: NaN/Inf found in RGB image display for sample {i}\")\n",
    "                             img_rgb_disp = np.nan_to_num(img_rgb_disp) # Replace NaN/Inf with 0\n",
    "                        plt.imshow(img_rgb_disp)\n",
    "                    else: # Mask\n",
    "                        if np.isnan(item).any() or np.isinf(item).any():\n",
    "                             print(f\"Warning: NaN/Inf found in mask display for sample {i}\")\n",
    "                             item = np.nan_to_num(item) # Replace NaN/Inf with 0\n",
    "                        plt.imshow(item, cmap='jet', vmin=0, vmax=1) # Use 'jet' or 'viridis' for mask\n",
    "\n",
    "                    plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout(pad=0.5) # Add padding\n",
    "        save_path = os.path.join(output_dir, \"dataset_visualization.png\")\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close() # Close plot to free memory\n",
    "        print(f\"Dataset visualization saved to {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during dataset visualization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda41a3",
   "metadata": {},
   "source": [
    "## Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1100f4e2",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ConciseProgressCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Logs progress concisely and performs garbage collection.\"\"\"\n",
    "    def __init__(self, log_frequency=1): # Log every epoch by default\n",
    "        super().__init__()\n",
    "        self.log_frequency = log_frequency\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "        if (epoch + 1) % self.log_frequency == 0:\n",
    "             print(f\"\\n--- Epoch {epoch + 1}/{self.params['epochs']} ---\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        if (epoch + 1) % self.log_frequency == 0:\n",
    "            metrics_str = \" - \".join([f\"{k}: {v:.4f}\" for k, v in logs.items()])\n",
    "            print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f}s - {metrics_str}\")\n",
    "\n",
    "            # Monitor Synergy weights\n",
    "            try:\n",
    "                synergy_layer = self.model.get_layer('synergy')\n",
    "                alpha_val = synergy_layer.alpha.numpy()\n",
    "                beta_val = synergy_layer.beta.numpy()\n",
    "                print(f\"    Synergy weights - alpha: {alpha_val:.4f}, beta: {beta_val:.4f}\")\n",
    "            except Exception as e:\n",
    "                # Layer might not exist if model architecture changes\n",
    "                pass # print(f\"    Could not get Synergy weights: {e}\")\n",
    "\n",
    "        # Force garbage collection at end of epoch\n",
    "        gc.collect()\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        total_time = time.time() - self.start_time\n",
    "        print(f\"\\n--- Training Finished ---\")\n",
    "        print(f\"Total training time: {total_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Learning Rate Scheduler Function\n",
    "def lr_step_decay(epoch, lr):\n",
    "    \"\"\"Applies step decay to the learning rate.\"\"\"\n",
    "    initial_lr = LEARNING_RATE # Use the initial LEARNING_RATE constant\n",
    "    drop = 0.5       # Factor to drop learning rate by\n",
    "    epochs_drop = 10  # Number of epochs before dropping LR\n",
    "    new_lr = initial_lr * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "    # Add a minimum learning rate limit\n",
    "    final_lr = max(new_lr, 1e-7)\n",
    "    # print(f\"Epoch {epoch+1}: LR = {final_lr:.7f}\") # Optional: print LR each epoch\n",
    "    return final_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed6b6c",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e1f42f1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_model_distributed(\n",
    "    dataset_func=prepare_brats_data_gpu,\n",
    "    model_func=AS_Net,\n",
    "    strategy=strategy, # Pass the distribution strategy\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    checkpoint_best_path=CHECKPOINT_BEST_PATH,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    img_height=IMG_HEIGHT,\n",
    "    img_width=IMG_WIDTH,\n",
    "    input_channels=INPUT_CHANNELS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    combined_loss_weights=COMBINED_LOSS_WEIGHTS, # Use constant from top\n",
    "    # Alternative: Use Focal Loss\n",
    "    # use_focal_loss=USE_FOCAL_LOSS,\n",
    "    metrics=['binary_accuracy',\n",
    "             DiceCoefficient(name='dice_coef'),\n",
    "             IoU(name='iou'),\n",
    "             tf.keras.metrics.Precision(thresholds=THRESHOLD, name=\"precision\"), # Add Precision/Recall here\n",
    "             tf.keras.metrics.Recall(thresholds=THRESHOLD, name=\"recall\")],\n",
    "):\n",
    "    \"\"\"Trains the AS-Net model using tf.distribute.Strategy.\"\"\"\n",
    "\n",
    "    # Calculate global batch size\n",
    "    global_batch_size = batch_size * strategy.num_replicas_in_sync\n",
    "    print(f\"--- Starting Training ---\")\n",
    "    print(f\"Number of replicas: {strategy.num_replicas_in_sync}\")\n",
    "    print(f\"Global batch size: {global_batch_size}\")\n",
    "    print(f\"Epochs: {num_epochs}\")\n",
    "    print(f\"Initial Learning Rate: {learning_rate}\")\n",
    "    print(f\"Loss configuration: {combined_loss_weights}\")\n",
    "\n",
    "\n",
    "    # 1. Create Datasets within Strategy Scope\n",
    "    print(\"Preparing datasets...\")\n",
    "    # Pass global_batch_size to the dataset function\n",
    "    train_dataset, val_dataset = dataset_func(batch_size=global_batch_size)\n",
    "    print(\"Datasets prepared.\")\n",
    "\n",
    "    # 2. Build and Compile Model within Strategy Scope\n",
    "    with strategy.scope():\n",
    "        print(\"Building model...\")\n",
    "        model = model_func(input_size=(img_height, img_width, input_channels))\n",
    "        print(\"Model built.\")\n",
    "\n",
    "        print(\"Compiling model...\")\n",
    "        # Instantiate loss function within strategy scope\n",
    "        # if use_focal_loss:\n",
    "        #      print(\"Using Combined Focal + Dice Loss\")\n",
    "        #      loss_instance = CombinedFocalDiceLoss(\n",
    "        #           focal_weight=combined_loss_weights['focal_weight'],\n",
    "        #           dice_weight=combined_loss_weights['dice_weight'],\n",
    "        #           focal_gamma=combined_loss_weights['focal_gamma']\n",
    "        #      )\n",
    "        # else:\n",
    "        print(\"Using Combined WBCE + Dice Loss\")\n",
    "        loss_instance = CombinedLoss(\n",
    "             bce_weight=combined_loss_weights['bce_weight'],\n",
    "             dice_weight=combined_loss_weights['dice_weight'],\n",
    "             class_weight=combined_loss_weights['class_weight']\n",
    "        )\n",
    "\n",
    "        # Instantiate optimizer within strategy scope\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        # Apply loss scaling for mixed precision if enabled\n",
    "        if mixed_precision.global_policy().name == 'mixed_float16':\n",
    "             optimizer = mixed_precision.LossScaleOptimizer(optimizer)\n",
    "             print(\"Loss scaling applied for mixed precision.\")\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_instance,\n",
    "            metrics=metrics # Use metrics list defined in function signature\n",
    "        )\n",
    "        print(\"Model compiled.\")\n",
    "        model.summary(line_length=120) # Print model summary\n",
    "\n",
    "    # Check if a checkpoint exists to resume training\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(os.path.dirname(checkpoint_path))\n",
    "    initial_epoch = 0\n",
    "    if latest_checkpoint:\n",
    "        print(f\"Resuming training from checkpoint: {latest_checkpoint}\")\n",
    "        try:\n",
    "            model.load_weights(latest_checkpoint).expect_partial() # Use expect_partial for optimizer state flexibility\n",
    "            # Extract epoch number from checkpoint filename if possible (e.g., 'ckpt-10.h5')\n",
    "            try:\n",
    "                 # Assuming format like model_.10-0.123456.h5 or ckpt-10.h5 etc.\n",
    "                 filename = os.path.basename(latest_checkpoint)\n",
    "                 epoch_str = filename.split('-')[0].split('.')[-1] # Try common patterns\n",
    "                 if not epoch_str.isdigit(): # Try another pattern if first fails\n",
    "                      epoch_str = filename.split('-')[0].split('_')[-1]\n",
    "                 initial_epoch = int(epoch_str)\n",
    "                 print(f\"Successfully loaded weights. Starting from epoch {initial_epoch + 1}\")\n",
    "            except Exception as parse_err:\n",
    "                 print(f\"Warning: Could not determine epoch number from checkpoint name '{latest_checkpoint}', starting from epoch 0. Error: {parse_err}\")\n",
    "                 initial_epoch = 0 # Default if parsing fails\n",
    "        except Exception as load_err:\n",
    "            print(f\"Error loading weights from {latest_checkpoint}: {load_err}. Starting training from scratch.\")\n",
    "            initial_epoch = 0\n",
    "\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting training from scratch.\")\n",
    "\n",
    "\n",
    "    # 3. Define Callbacks\n",
    "    callbacks = [\n",
    "        # Save weights only, save best based on val_dice_coef (higher is better)\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_best_path,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_dice_coef', # Monitor Dice Coefficient\n",
    "            mode='max',             # Save model with highest Dice\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        # Save weights every epoch (for resuming)\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_path, # Overwrites each epoch\n",
    "            save_weights_only=True,\n",
    "            save_freq='epoch',      # Save at the end of every epoch\n",
    "            verbose=0\n",
    "        ),\n",
    "        # Reduce learning rate on plateau\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',     # Monitor validation loss\n",
    "            factor=0.5,             # Reduce LR by half\n",
    "            patience=5,             # Wait 5 epochs with no improvement\n",
    "            min_lr=1e-7,            # Minimum learning rate\n",
    "            verbose=1\n",
    "        ),\n",
    "        # Early stopping\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',     # Monitor validation loss\n",
    "            patience=15,            # Wait 15 epochs with no improvement\n",
    "            restore_best_weights=True, # Restore weights from best epoch (based on val_loss)\n",
    "            verbose=1\n",
    "        ),\n",
    "        # Apply learning rate schedule\n",
    "        tf.keras.callbacks.LearningRateScheduler(lr_step_decay, verbose=0),\n",
    "        # Custom progress logger\n",
    "        ConciseProgressCallback(log_frequency=1),\n",
    "        # TensorBoard (optional)\n",
    "        # tf.keras.callbacks.TensorBoard(log_dir=os.path.join(output_dir, 'logs'), histogram_freq=1)\n",
    "    ]\n",
    "\n",
    "    # 4. Train the Model\n",
    "    print(f\"Starting training loop for {num_epochs - initial_epoch} potential epochs (may stop early)...\")\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=num_epochs,\n",
    "        initial_epoch=initial_epoch, # Start from the correct epoch if resuming\n",
    "        callbacks=callbacks,\n",
    "        verbose=0 # Use custom callback for progress logging\n",
    "    )\n",
    "\n",
    "    # 5. Save Training History\n",
    "    try:\n",
    "        hist_df = pd.DataFrame(history.history)\n",
    "        hist_csv_file = os.path.join(output_dir, 'training_history.csv')\n",
    "        hist_df.to_csv(hist_csv_file, index=False)\n",
    "        print(f\"Training history saved to {hist_csv_file}\")\n",
    "\n",
    "        # Plot history and save plots\n",
    "        plot_training_history(history, output_dir)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving training history or plotting: {e}\")\n",
    "\n",
    "    # Force cleanup after training\n",
    "    print(\"Cleaning up resources after training attempt...\")\n",
    "    del train_dataset, val_dataset # Explicitly delete datasets\n",
    "    gc.collect()\n",
    "    backend.clear_session() # Clear Keras session\n",
    "    print(\"Cleaned up datasets and session.\")\n",
    "\n",
    "    return history, model # Return the final model state (might be from early stopping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceae1f4",
   "metadata": {},
   "source": [
    "## Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efb49855",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history, output_dir=OUTPUT_DIR):\n",
    "    \"\"\"Plots training & validation loss and metrics and saves the plot.\"\"\"\n",
    "    print(\"--- Plotting Training History ---\")\n",
    "    try:\n",
    "        history_dict = history.history\n",
    "        if not history_dict:\n",
    "             print(\"History object is empty. Skipping plotting.\")\n",
    "             return\n",
    "\n",
    "        epochs = range(1, len(history_dict['loss']) + 1)\n",
    "\n",
    "        # Determine available metrics (handle potential missing keys)\n",
    "        metrics_to_plot = {'loss': 'Loss'}\n",
    "        # Check for custom metrics and standard ones\n",
    "        for key in history_dict.keys():\n",
    "            if key.startswith('val_'): continue # Skip val metrics here\n",
    "            if key == 'loss': continue\n",
    "            if key == 'dice_coef': metrics_to_plot['dice_coef'] = 'Dice Coefficient'\n",
    "            elif key == 'iou': metrics_to_plot['iou'] = 'IoU (Jaccard)'\n",
    "            elif key == 'binary_accuracy': metrics_to_plot['binary_accuracy'] = 'Binary Accuracy'\n",
    "            elif key == 'precision': metrics_to_plot['precision'] = 'Precision'\n",
    "            elif key == 'recall': metrics_to_plot['recall'] = 'Recall'\n",
    "            # Add other potential metrics if needed\n",
    "\n",
    "        num_plots = len(metrics_to_plot)\n",
    "        if num_plots == 0:\n",
    "            print(\"No metrics found to plot (excluding loss).\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(6 * num_plots, 5))\n",
    "\n",
    "        plot_index = 1\n",
    "        for metric, title in metrics_to_plot.items():\n",
    "            plt.subplot(1, num_plots, plot_index)\n",
    "            # Plot training metric\n",
    "            if metric in history_dict:\n",
    "                plt.plot(epochs, history_dict[metric], 'bo-', label=f'Training {title}')\n",
    "            else:\n",
    "                 print(f\"Warning: Training metric '{metric}' not found in history.\")\n",
    "\n",
    "            # Plot validation metric\n",
    "            val_metric = f'val_{metric}'\n",
    "            if val_metric in history_dict:\n",
    "                plt.plot(epochs, history_dict[val_metric], 'ro-', label=f'Validation {title}')\n",
    "            else:\n",
    "                 print(f\"Warning: Validation metric '{val_metric}' not found in history.\")\n",
    "\n",
    "\n",
    "            plt.title(f'Training and Validation {title}')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel(title)\n",
    "            # Set y-axis limits appropriately\n",
    "            if metric != 'loss':\n",
    "                 plt.ylim([0, 1]) # Limit Dice, IoU, Acc, etc. to [0, 1]\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plot_index += 1\n",
    "\n",
    "        plt.tight_layout(pad=1.0)\n",
    "        save_path = os.path.join(output_dir, \"training_history_plots.png\")\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close() # Close plot to free memory\n",
    "        print(f\"Training history plots saved to {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting training history: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e94f6d7",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8c34cdd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model=None, # Pass the trained model or load from checkpoint\n",
    "    checkpoint_best_path=CHECKPOINT_BEST_PATH,\n",
    "    output_folder=OUTPUT_DIR,\n",
    "    img_height=IMG_HEIGHT,\n",
    "    img_width=IMG_WIDTH,\n",
    "    input_channels=INPUT_CHANNELS,\n",
    "    batch_size=BATCH_SIZE, # Use evaluation batch size\n",
    "    dataset_func=prepare_brats_data_gpu,\n",
    "    threshold=THRESHOLD,\n",
    "    num_examples_to_save=5,\n",
    "    loss_config=COMBINED_LOSS_WEIGHTS, # Pass loss config\n",
    "    # use_focal_loss=USE_FOCAL_LOSS, # Pass loss choice\n",
    "):\n",
    "    \"\"\"Evaluates the trained AS-Net model on the validation set.\"\"\"\n",
    "    print(\"\\n--- Starting Model Evaluation ---\")\n",
    "    evaluation_results = None # Initialize results\n",
    "\n",
    "    try:\n",
    "        # 1. Load Validation Data\n",
    "        # Use global batch size for evaluation dataset as well\n",
    "        global_eval_batch_size = batch_size * strategy.num_replicas_in_sync\n",
    "        print(f\"Loading validation data with batch size: {global_eval_batch_size}...\")\n",
    "        _, val_dataset = dataset_func(batch_size=global_eval_batch_size, validation_split=0.2) # Ensure consistent split\n",
    "        print(\"Validation dataset loaded.\")\n",
    "\n",
    "        # 2. Load or Use Provided Model\n",
    "        if model is None:\n",
    "             print(f\"Loading model weights from best checkpoint: {checkpoint_best_path}\")\n",
    "             if not os.path.exists(checkpoint_best_path):\n",
    "                  print(f\"Error: Best checkpoint file not found at {checkpoint_best_path}.\")\n",
    "                  # Try loading the last epoch checkpoint instead\n",
    "                  last_checkpoint = tf.train.latest_checkpoint(os.path.dirname(checkpoint_path))\n",
    "                  if not last_checkpoint:\n",
    "                      print(f\"Error: No checkpoint found in {os.path.dirname(checkpoint_path)}. Cannot evaluate.\")\n",
    "                      return None # Cannot proceed without a model\n",
    "                  else:\n",
    "                      print(f\"Warning: Best checkpoint not found. Loading last epoch checkpoint: {last_checkpoint}\")\n",
    "                      checkpoint_to_load = last_checkpoint\n",
    "             else:\n",
    "                  checkpoint_to_load = checkpoint_best_path\n",
    "\n",
    "             # Build model architecture again before loading weights\n",
    "             print(\"Rebuilding model architecture for evaluation...\")\n",
    "             with strategy.scope(): # Load model within strategy scope if needed\n",
    "                 model_eval = AS_Net(input_size=(img_height, img_width, input_channels))\n",
    "\n",
    "                 # Compile is necessary to run evaluate, use the same loss/metrics as training\n",
    "                 print(\"Compiling evaluation model...\")\n",
    "                 # Instantiate loss function\n",
    "                 # if use_focal_loss:\n",
    "                 #      print(\"Using Combined Focal + Dice Loss for eval compilation\")\n",
    "                 #      loss_instance = CombinedFocalDiceLoss(\n",
    "                 #           focal_weight=loss_config['focal_weight'],\n",
    "                 #           dice_weight=loss_config['dice_weight'],\n",
    "                 #           focal_gamma=loss_config['focal_gamma']\n",
    "                 #      )\n",
    "                 # else:\n",
    "                 print(\"Using Combined WBCE + Dice Loss for eval compilation\")\n",
    "                 loss_instance = CombinedLoss(\n",
    "                      bce_weight=loss_config['bce_weight'],\n",
    "                      dice_weight=loss_config['dice_weight'],\n",
    "                      class_weight=loss_config['class_weight']\n",
    "                 )\n",
    "                 # Instantiate optimizer (not used for eval weights, but needed for compile)\n",
    "                 optimizer = tf.keras.optimizers.Adam()\n",
    "                 # Apply loss scaling if needed\n",
    "                 if mixed_precision.global_policy().name == 'mixed_float16':\n",
    "                    optimizer = mixed_precision.LossScaleOptimizer(optimizer)\n",
    "\n",
    "                 model_eval.compile(\n",
    "                      optimizer=optimizer,\n",
    "                      loss=loss_instance,\n",
    "                      metrics=['binary_accuracy',\n",
    "                               DiceCoefficient(name='dice_coef'), # Ensure metrics are instantiated\n",
    "                               IoU(name='iou'),\n",
    "                               tf.keras.metrics.Precision(thresholds=threshold, name=\"precision\"),\n",
    "                               tf.keras.metrics.Recall(thresholds=threshold, name=\"recall\")]\n",
    "                 )\n",
    "                 print(\"Evaluation model compiled.\")\n",
    "                 print(f\"Loading weights from {checkpoint_to_load}...\")\n",
    "                 model_eval.load_weights(checkpoint_to_load).expect_partial() # Use expect_partial\n",
    "                 print(f\"Successfully loaded weights into new model instance from {checkpoint_to_load}\")\n",
    "        else:\n",
    "            print(\"Using provided trained model instance for evaluation.\")\n",
    "            # Ensure the provided model is compiled with the necessary metrics if not already done\n",
    "            if not model.optimizer:\n",
    "                 print(\"Compiling the provided model for evaluation...\")\n",
    "                 with strategy.scope():\n",
    "                      # Compile similarly to how it's done when loading from checkpoint\n",
    "                      # Instantiate loss function\n",
    "                      # if use_focal_loss: loss_instance = CombinedFocalDiceLoss(**loss_config)\n",
    "                      # else: loss_instance = CombinedLoss(**loss_config)\n",
    "                      loss_instance = CombinedLoss(**loss_config) # Assuming WBCE+Dice if not focal\n",
    "\n",
    "                      optimizer = tf.keras.optimizers.Adam()\n",
    "                      if mixed_precision.global_policy().name == 'mixed_float16': optimizer = mixed_precision.LossScaleOptimizer(optimizer)\n",
    "\n",
    "                      model.compile(\n",
    "                           optimizer=optimizer, loss=loss_instance,\n",
    "                           metrics=['binary_accuracy', DiceCoefficient(name='dice_coef'), IoU(name='iou'),\n",
    "                                    tf.keras.metrics.Precision(thresholds=threshold, name=\"precision\"),\n",
    "                                    tf.keras.metrics.Recall(thresholds=threshold, name=\"recall\")]\n",
    "                      )\n",
    "                      print(\"Provided model compiled.\")\n",
    "            model_eval = model # Use the model passed from training\n",
    "\n",
    "        # 3. Evaluate using model.evaluate()\n",
    "        print(\"Evaluating model on validation set...\")\n",
    "        evaluation_results = model_eval.evaluate(val_dataset, verbose=1, return_dict=True)\n",
    "        print(\"\\nKeras Evaluation Results:\")\n",
    "        for name, value in evaluation_results.items():\n",
    "            print(f\"- {name}: {value:.4f}\")\n",
    "\n",
    "        # 4. Calculate F1 Score (using precision and recall from Keras metrics)\n",
    "        precision_val = evaluation_results.get('precision', 0.0)\n",
    "        recall_val = evaluation_results.get('recall', 0.0)\n",
    "        if (precision_val + recall_val) > 1e-7: # Avoid division by zero\n",
    "            f1_val = 2 * (precision_val * recall_val) / (precision_val + recall_val)\n",
    "        else:\n",
    "            f1_val = 0.0\n",
    "        evaluation_results['f1_score'] = f1_val # Add F1 to the results dictionary\n",
    "        print(f\"- f1_score: {f1_val:.4f} (calculated from Precision/Recall)\")\n",
    "\n",
    "        # 5. Save Performance Metrics to File\n",
    "        try:\n",
    "            perf_file_path = os.path.join(output_folder, \"performances.txt\")\n",
    "            with open(perf_file_path, \"w\") as file_perf:\n",
    "                file_perf.write(\"Evaluation Metrics:\\n\")\n",
    "                file_perf.write(\"-------------------\\n\")\n",
    "                file_perf.write(f\"Loss: {evaluation_results.get('loss', 'N/A'):.4f}\\n\")\n",
    "                file_perf.write(f\"Binary Accuracy: {evaluation_results.get('binary_accuracy', 'N/A'):.4f}\\n\")\n",
    "                file_perf.write(f\"Dice Coefficient: {evaluation_results.get('dice_coef', 'N/A'):.4f}\\n\")\n",
    "                file_perf.write(f\"IoU (Jaccard): {evaluation_results.get('iou', 'N/A'):.4f}\\n\")\n",
    "                file_perf.write(f\"Precision (Threshold={threshold}): {evaluation_results.get('precision', 'N/A'):.4f}\\n\")\n",
    "                file_perf.write(f\"Recall/Sensitivity (Threshold={threshold}): {evaluation_results.get('recall', 'N/A'):.4f}\\n\")\n",
    "                file_perf.write(f\"F1-Score (Threshold={threshold}): {evaluation_results.get('f1_score', 'N/A'):.4f}\\n\")\n",
    "            print(f\"Evaluation results saved to {perf_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving performance metrics to file: {e}\")\n",
    "\n",
    "        # 6. Generate and Save Prediction Examples\n",
    "        print(\"\\nGenerating prediction examples...\")\n",
    "        # Ensure the model used for prediction is the evaluated one\n",
    "        save_prediction_examples(model_eval, val_dataset, output_folder, num_examples=num_examples_to_save, threshold=threshold)\n",
    "\n",
    "        print(\"--- Evaluation Finished ---\")\n",
    "        return evaluation_results # Return the metrics dictionary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None # Return None if evaluation failed\n",
    "    finally:\n",
    "        # Final cleanup\n",
    "        print(\"Cleaning up resources after evaluation...\")\n",
    "        if 'val_dataset' in locals(): del val_dataset # Delete dataset\n",
    "        if 'model_eval' in locals() and model_eval is not model: del model_eval # Delete model if loaded here\n",
    "        gc.collect()\n",
    "        # Avoid clearing session if model was passed in and might be used later\n",
    "        # backend.clear_session()\n",
    "        print(\"Cleaned up evaluation resources.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aaa7ec",
   "metadata": {},
   "source": [
    "## Save Prediction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd46f879",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_prediction_examples(model, dataset, output_folder, num_examples=5, threshold=THRESHOLD):\n",
    "    \"\"\"Saves example predictions with inputs and ground truth.\"\"\"\n",
    "    print(f\"Saving {num_examples} prediction examples...\")\n",
    "    examples_dir = os.path.join(output_folder, \"examples\")\n",
    "    os.makedirs(examples_dir, exist_ok=True) # Ensure directory exists\n",
    "\n",
    "    try:\n",
    "        # Take one batch from the dataset\n",
    "        for images, masks in dataset.take(1):\n",
    "            print(f\"Generating predictions for {min(num_examples, images.shape[0])} examples...\")\n",
    "            # Predict using the model\n",
    "            predictions = model.predict(images)\n",
    "            # Apply threshold to get binary predictions\n",
    "            binary_predictions = tf.cast(predictions >= threshold, tf.float32).numpy()\n",
    "            predictions = predictions.numpy() # Get probability maps as numpy array\n",
    "            images = images.numpy()\n",
    "            masks = masks.numpy()\n",
    "\n",
    "            print(\"Plotting and saving examples...\")\n",
    "            for j in range(min(num_examples, images.shape[0])):\n",
    "                plt.figure(figsize=(16, 4)) # Width adjusted for 4 plots\n",
    "\n",
    "                # --- Plot 1: Original RGB Input ---\n",
    "                plt.subplot(1, 4, 1)\n",
    "                plt.title(\"Input RGB\")\n",
    "                # Clip Z-score normalized data to [0,1] for display\n",
    "                img_display = np.clip(images[j], 0, 1)\n",
    "                if np.isnan(img_display).any() or np.isinf(img_display).any():\n",
    "                     print(f\"Warning: NaN/Inf found in input image display for example {j}\")\n",
    "                     img_display = np.nan_to_num(img_display)\n",
    "                plt.imshow(img_display)\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                # --- Plot 2: Ground Truth Mask ---\n",
    "                plt.subplot(1, 4, 2)\n",
    "                plt.title(\"Ground Truth Mask\")\n",
    "                # Display background image slightly faded\n",
    "                plt.imshow(img_display, cmap='gray', alpha=0.6)\n",
    "                # Overlay mask\n",
    "                gt_mask_display = masks[j].squeeze()\n",
    "                if np.isnan(gt_mask_display).any() or np.isinf(gt_mask_display).any():\n",
    "                     print(f\"Warning: NaN/Inf found in GT mask display for example {j}\")\n",
    "                     gt_mask_display = np.nan_to_num(gt_mask_display)\n",
    "                plt.imshow(gt_mask_display, cmap='viridis', alpha=0.5, vmin=0, vmax=1) # Use squeeze for mask (H,W)\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                # --- Plot 3: Prediction Probability Map ---\n",
    "                plt.subplot(1, 4, 3)\n",
    "                plt.title(\"Prediction Probabilities\")\n",
    "                plt.imshow(img_display, cmap='gray', alpha=0.6)\n",
    "                # Overlay probability map\n",
    "                pred_prob_display = predictions[j].squeeze()\n",
    "                if np.isnan(pred_prob_display).any() or np.isinf(pred_prob_display).any():\n",
    "                     print(f\"Warning: NaN/Inf found in probability map display for example {j}\")\n",
    "                     pred_prob_display = np.nan_to_num(pred_prob_display)\n",
    "                prob_map = plt.imshow(pred_prob_display, cmap='hot', alpha=0.5, vmin=0, vmax=1)\n",
    "                plt.axis(\"off\")\n",
    "                # Add colorbar for probability map\n",
    "                # plt.colorbar(prob_map, fraction=0.046, pad=0.04) # Optional: Add colorbar\n",
    "\n",
    "                # --- Plot 4: Binary Prediction (Thresholded) ---\n",
    "                plt.subplot(1, 4, 4)\n",
    "                plt.title(f\"Binary Prediction (t={threshold:.2f})\")\n",
    "                plt.imshow(img_display, cmap='gray', alpha=0.6)\n",
    "                # Overlay binary prediction\n",
    "                binary_pred_display = binary_predictions[j].squeeze()\n",
    "                if np.isnan(binary_pred_display).any() or np.isinf(binary_pred_display).any():\n",
    "                     print(f\"Warning: NaN/Inf found in binary prediction display for example {j}\")\n",
    "                     binary_pred_display = np.nan_to_num(binary_pred_display)\n",
    "                plt.imshow(binary_pred_display, cmap='viridis', alpha=0.5, vmin=0, vmax=1)\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                # Save the figure\n",
    "                plt.tight_layout(pad=0.5) # Add padding\n",
    "                example_save_path = os.path.join(examples_dir, f\"prediction_example_{j+1}.png\")\n",
    "                plt.savefig(example_save_path, dpi=150, bbox_inches='tight')\n",
    "                plt.close() # Close the figure to free memory\n",
    "\n",
    "            print(f\"Saved prediction examples to {examples_dir}\")\n",
    "            break # Only process one batch\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving prediction examples: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b1f60a",
   "metadata": {},
   "source": [
    "## Completion Notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "733c6aa8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_completion_notification(output_folder=OUTPUT_DIR, completion_file=COMPLETION_FILE, start_time=None):\n",
    "    \"\"\"Creates a text file summarizing the training run and results.\"\"\"\n",
    "    print(\"\\n--- Creating Completion Notification ---\")\n",
    "    import datetime\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    perf_file_path = os.path.join(output_folder, \"performances.txt\")\n",
    "    \n",
    "    # Calculate duration if start_time was provided\n",
    "    duration_str = \"Unknown (start time not recorded)\"\n",
    "    if start_time is not None:\n",
    "        # Calculate duration in seconds\n",
    "        duration_seconds = time.time() - start_time\n",
    "        # Convert to hours and minutes\n",
    "        hours = int(duration_seconds // 3600)\n",
    "        minutes = int((duration_seconds % 3600) // 60)\n",
    "        seconds = int(duration_seconds % 60)\n",
    "        duration_str = f\"{hours}h {minutes}m {seconds}s\"\n",
    "\n",
    "    try:\n",
    "        with open(completion_file, \"w\") as f:\n",
    "            f.write(f\"VGG16 AS-Net Training Completed at: {timestamp}\\n\\n\")\n",
    "            f.write(\"Training Configuration:\\n\")\n",
    "            f.write(f\"- Model: AS-Net with VGG16 encoder\\n\")\n",
    "            f.write(f\"- Image dimensions: {IMG_HEIGHT}x{IMG_WIDTH}\\n\")\n",
    "            f.write(f\"- Input Channels: {INPUT_CHANNELS}\\n\")\n",
    "            f.write(f\"- Batch size (per replica): {BATCH_SIZE}\\n\")\n",
    "            f.write(f\"- Global Batch size: {BATCH_SIZE * strategy.num_replicas_in_sync}\\n\")\n",
    "            f.write(f\"- Epochs planned: {NUM_EPOCHS}\\n\")\n",
    "            f.write(f\"- Initial Learning rate: {LEARNING_RATE}\\n\")\n",
    "            f.write(f\"- Mixed Precision Policy: {mixed_precision.global_policy().name}\\n\")\n",
    "            f.write(f\"- Loss Config: {COMBINED_LOSS_WEIGHTS}\\n\")\n",
    "            f.write(f\"- Total Duration: {duration_str}\\n\\n\")\n",
    "            # f.write(f\"- Using Focal Loss: {USE_FOCAL_LOSS}\\n\\n\") # If using focal loss toggle\n",
    "\n",
    "            f.write(\"Checkpoint and output locations:\\n\")\n",
    "            f.write(f\"- Checkpoint directory: {CHECKPOINT_DIR}\\n\")\n",
    "            f.write(f\"- Best model weights: {CHECKPOINT_BEST_PATH}\\n\")\n",
    "            f.write(f\"- Output directory: {output_folder}\\n\")\n",
    "\n",
    "            # Add final performance metrics if available\n",
    "            f.write(\"\\n--- Final Performance Metrics ---\\n\")\n",
    "            if os.path.exists(perf_file_path):\n",
    "                try:\n",
    "                    with open(perf_file_path, \"r\") as perf_file:\n",
    "                        f.write(perf_file.read())\n",
    "                except Exception as read_err:\n",
    "                    f.write(f\"Note: Error reading performance metrics file ({perf_file_path}): {read_err}\\n\")\n",
    "            else:\n",
    "                f.write(f\"Note: Performance metrics file not found ({perf_file_path}). Evaluation might have failed or not run yet.\\n\")\n",
    "\n",
    "        print(f\"Completion notification saved to: {completion_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating completion notification file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6cb685",
   "metadata": {},
   "source": [
    "## Execute Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe06c6ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion file not found. Starting training process...\n",
      "--- Starting Training ---\n",
      "Number of replicas: 1\n",
      "Global batch size: 4\n",
      "Epochs: 30\n",
      "Initial Learning Rate: 0.0001\n",
      "Loss configuration: {'bce_weight': 0.5, 'dice_weight': 0.5, 'class_weight': 100.0}\n",
      "Preparing datasets...\n",
      "--- Setting up Data Pipeline ---\n",
      "Loading metadata from: brats2020-training-data/BraTS20 Training Metadata.csv\n",
      "Loading H5 data from: brats2020-training-data/BraTS2020_training_data/content/data\n",
      "Target image size: (192, 192)\n",
      "Modalities loaded (indices): [1, 3]\n",
      "Modalities mapped to RGB (indices): [0, 1, 0]\n",
      "Found 57195 existing H5 files referenced in metadata.\n",
      "Training samples: 45756, Validation samples: 11439\n",
      "Data pipeline created successfully.\n",
      "Datasets prepared.\n",
      "Building model...\n",
      "Unfroze VGG16 layers from block4 onwards for fine-tuning.\n",
      "Model built.\n",
      "Compiling model...\n",
      "Using Combined WBCE + Dice Loss\n",
      "Model compiled.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"AS_Net_VGG16\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"AS_Net_VGG16\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                      </span><span style=\"font-weight: bold\"> Output Shape                 </span><span style=\"font-weight: bold\">           Param # </span><span style=\"font-weight: bold\"> Connected to              </span>\n",
       "\n",
       " input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                         \n",
       "\n",
       " block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
       "\n",
       " block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span>  block1_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "\n",
       " block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  block1_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "\n",
       " block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span>  block1_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
       "\n",
       " block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span>  block2_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "\n",
       " block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  block2_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "\n",
       " block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span>  block2_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
       "\n",
       " block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span>  block3_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "\n",
       " block3_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span>  block3_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "\n",
       " block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  block3_conv3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "\n",
       " block4_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span>  block3_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
       "\n",
       " block4_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span>  block4_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "\n",
       " block4_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span>  block4_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "\n",
       " block4_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  block4_conv3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "\n",
       " block5_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span>  block4_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
       "\n",
       " block5_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span>  block5_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "\n",
       " block5_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span>  block5_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "\n",
       " up5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  block5_conv3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        \n",
       "\n",
       " merge1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  block4_conv3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], up5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]\n",
       "\n",
       " sam1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SAM</span>)                         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">3,933,696</span>  merge1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              \n",
       "\n",
       " cam1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CAM</span>)                         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">3,810,304</span>  merge1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              \n",
       "\n",
       " up_sam1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  sam1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                \n",
       "\n",
       " up_cam1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  cam1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                \n",
       "\n",
       " merge21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  block3_conv3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
       "                                                                                     up_sam1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "\n",
       " merge22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  block3_conv3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
       "                                                                                     up_cam1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "\n",
       " sam2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SAM</span>)                         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">983,808</span>  merge21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "\n",
       " cam2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CAM</span>)                         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">952,832</span>  merge22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "\n",
       " up_sam2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  sam2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                \n",
       "\n",
       " up_cam2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  cam2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                \n",
       "\n",
       " merge31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  block2_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
       "                                                                                     up_sam2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "\n",
       " merge32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  block2_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
       "                                                                                     up_cam2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "\n",
       " sam3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SAM</span>)                         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">246,144</span>  merge31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "\n",
       " cam3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CAM</span>)                         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">238,336</span>  merge32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "\n",
       " up_sam3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  sam3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                \n",
       "\n",
       " up_cam3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  cam3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]                \n",
       "\n",
       " merge41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  block1_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
       "                                                                                     up_sam3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "\n",
       " merge42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  block1_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
       "                                                                                     up_cam3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "\n",
       " sam4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SAM</span>)                         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">61,632</span>  merge41[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "\n",
       " cam4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CAM</span>)                         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">59,648</span>  merge42[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "\n",
       " synergy (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Synergy</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>  sam4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], cam4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n",
       "\n",
       " final_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>  synergy[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]             \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape                \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m          Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\n",
       "\n",
       " input_layer (\u001b[38;5;33mInputLayer\u001b[0m)           (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m3\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  -                         \n",
       "\n",
       " block1_conv1 (\u001b[38;5;33mConv2D\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)                      \u001b[38;5;34m1,792\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
       "\n",
       " block1_conv2 (\u001b[38;5;33mConv2D\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)                     \u001b[38;5;34m36,928\u001b[0m  block1_conv1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "\n",
       " block1_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                            \u001b[38;5;34m0\u001b[0m  block1_conv2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "\n",
       " block2_conv1 (\u001b[38;5;33mConv2D\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                      \u001b[38;5;34m73,856\u001b[0m  block1_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
       "\n",
       " block2_conv2 (\u001b[38;5;33mConv2D\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                     \u001b[38;5;34m147,584\u001b[0m  block2_conv1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "\n",
       " block2_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  block2_conv2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "\n",
       " block3_conv1 (\u001b[38;5;33mConv2D\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)                     \u001b[38;5;34m295,168\u001b[0m  block2_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
       "\n",
       " block3_conv2 (\u001b[38;5;33mConv2D\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)                     \u001b[38;5;34m590,080\u001b[0m  block3_conv1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "\n",
       " block3_conv3 (\u001b[38;5;33mConv2D\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)                     \u001b[38;5;34m590,080\u001b[0m  block3_conv2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "\n",
       " block3_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  block3_conv3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "\n",
       " block4_conv1 (\u001b[38;5;33mConv2D\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   \u001b[38;5;34m1,180,160\u001b[0m  block3_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
       "\n",
       " block4_conv2 (\u001b[38;5;33mConv2D\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   \u001b[38;5;34m2,359,808\u001b[0m  block4_conv1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "\n",
       " block4_conv3 (\u001b[38;5;33mConv2D\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   \u001b[38;5;34m2,359,808\u001b[0m  block4_conv2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "\n",
       " block4_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m512\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  block4_conv3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "\n",
       " block5_conv1 (\u001b[38;5;33mConv2D\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   \u001b[38;5;34m2,359,808\u001b[0m  block4_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
       "\n",
       " block5_conv2 (\u001b[38;5;33mConv2D\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   \u001b[38;5;34m2,359,808\u001b[0m  block5_conv1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "\n",
       " block5_conv3 (\u001b[38;5;33mConv2D\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   \u001b[38;5;34m2,359,808\u001b[0m  block5_conv2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "\n",
       " up5 (\u001b[38;5;33mUpSampling2D\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m512\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  block5_conv3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        \n",
       "\n",
       " merge1 (\u001b[38;5;33mConcatenate\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  block4_conv3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], up5[\u001b[38;5;34m0\u001b[0m]\n",
       "\n",
       " sam1 (\u001b[38;5;33mSAM\u001b[0m)                         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                   \u001b[38;5;34m3,933,696\u001b[0m  merge1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              \n",
       "\n",
       " cam1 (\u001b[38;5;33mCAM\u001b[0m)                         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m256\u001b[0m)                   \u001b[38;5;34m3,810,304\u001b[0m  merge1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              \n",
       "\n",
       " up_sam1 (\u001b[38;5;33mUpSampling2D\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  sam1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                \n",
       "\n",
       " up_cam1 (\u001b[38;5;33mUpSampling2D\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m256\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  cam1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                \n",
       "\n",
       " merge21 (\u001b[38;5;33mConcatenate\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  block3_conv3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
       "                                                                                     up_sam1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "\n",
       " merge22 (\u001b[38;5;33mConcatenate\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m512\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  block3_conv3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
       "                                                                                     up_cam1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "\n",
       " sam2 (\u001b[38;5;33mSAM\u001b[0m)                         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                     \u001b[38;5;34m983,808\u001b[0m  merge21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "\n",
       " cam2 (\u001b[38;5;33mCAM\u001b[0m)                         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m128\u001b[0m)                     \u001b[38;5;34m952,832\u001b[0m  merge22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "\n",
       " up_sam2 (\u001b[38;5;33mUpSampling2D\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  sam2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                \n",
       "\n",
       " up_cam2 (\u001b[38;5;33mUpSampling2D\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  cam2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                \n",
       "\n",
       " merge31 (\u001b[38;5;33mConcatenate\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m256\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  block2_conv2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
       "                                                                                     up_sam2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "\n",
       " merge32 (\u001b[38;5;33mConcatenate\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m256\u001b[0m)                           \u001b[38;5;34m0\u001b[0m  block2_conv2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
       "                                                                                     up_cam2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "\n",
       " sam3 (\u001b[38;5;33mSAM\u001b[0m)                         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                      \u001b[38;5;34m246,144\u001b[0m  merge31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "\n",
       " cam3 (\u001b[38;5;33mCAM\u001b[0m)                         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m64\u001b[0m)                      \u001b[38;5;34m238,336\u001b[0m  merge32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "\n",
       " up_sam3 (\u001b[38;5;33mUpSampling2D\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  sam3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                \n",
       "\n",
       " up_cam3 (\u001b[38;5;33mUpSampling2D\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m64\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  cam3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]                \n",
       "\n",
       " merge41 (\u001b[38;5;33mConcatenate\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m128\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  block1_conv2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
       "                                                                                     up_sam3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "\n",
       " merge42 (\u001b[38;5;33mConcatenate\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m128\u001b[0m)                         \u001b[38;5;34m0\u001b[0m  block1_conv2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
       "                                                                                     up_cam3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "\n",
       " sam4 (\u001b[38;5;33mSAM\u001b[0m)                         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m32\u001b[0m)                     \u001b[38;5;34m61,632\u001b[0m  merge41[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "\n",
       " cam4 (\u001b[38;5;33mCAM\u001b[0m)                         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m32\u001b[0m)                     \u001b[38;5;34m59,648\u001b[0m  merge42[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "\n",
       " synergy (\u001b[38;5;33mSynergy\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m1\u001b[0m)                          \u001b[38;5;34m37\u001b[0m  sam4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], cam4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n",
       "\n",
       " final_output (\u001b[38;5;33mConv2D\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m192\u001b[0m, \u001b[38;5;34m1\u001b[0m)                           \u001b[38;5;34m2\u001b[0m  synergy[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]             \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,001,127</span> (95.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,001,127\u001b[0m (95.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,265,637</span> (88.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,265,637\u001b[0m (88.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,735,490</span> (6.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,735,490\u001b[0m (6.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, starting training from scratch.\n",
      "Starting training loop for 30 potential epochs (may stop early)...\n",
      "\n",
      "--- Epoch 1/30 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743375141.136654  284645 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    }
   ],
   "source": [
    "# Record start time for duration calculation\n",
    "script_start_time = time.time()\n",
    "\n",
    "# Step 1: Train the model (or load if already trained)\n",
    "# Check if completion file exists to skip training\n",
    "if os.path.exists(COMPLETION_FILE):\n",
    "     print(f\"Completion file '{COMPLETION_FILE}' found. Skipping training.\")\n",
    "     # Ensure the model architecture is defined even if skipping training,\n",
    "     # so evaluation can build it before loading weights.\n",
    "     # Build model within strategy scope\n",
    "     with strategy.scope():\n",
    "          model = AS_Net(input_size=(IMG_HEIGHT, IMG_WIDTH, INPUT_CHANNELS))\n",
    "          print(\"Model architecture defined for potential evaluation.\")\n",
    "     history = None # No history object if training is skipped\n",
    "else:\n",
    "     print(\"Completion file not found. Starting training process...\")\n",
    "     history, model = train_model_distributed() # history and model are returned\n",
    "\n",
    "# Step 2: Evaluate the model\n",
    "# Pass the model ONLY if training actually ran and returned a model\n",
    "model_to_evaluate = model if 'model' in locals() and model is not None else None\n",
    "evaluation_results = evaluate_model(model=model_to_evaluate) # Pass model if it exists\n",
    "\n",
    "# Step 3: Create completion notification (will include eval results if successful)\n",
    "create_completion_notification(start_time=script_start_time)\n",
    "\n",
    "# Final cleanup\n",
    "print(\"\\n--- Final Script Cleanup ---\")\n",
    "if 'model' in locals(): del model\n",
    "if 'train_dataset' in locals(): del train_dataset\n",
    "if 'val_dataset' in locals(): del val_dataset\n",
    "if 'evaluation_results' in locals(): del evaluation_results\n",
    "if 'history' in locals(): del history\n",
    "gc.collect()\n",
    "backend.clear_session()\n",
    "print(\"Script execution completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af537b1f-ebd9-47e8-bce2-10fd1b55a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
