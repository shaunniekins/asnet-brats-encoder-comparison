{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5edd8e",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86740a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy keras matplotlib scikit-learn pandas tensorflow[and-cuda] nvidia-cudnn-cu12 h5py -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe765070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import Model, Input, backend\n",
    "# Import EfficientNetV2 models and preprocessing\n",
    "from keras.applications import EfficientNetV2B0, EfficientNetV2B1, EfficientNetV2B2, EfficientNetV2B3 # Add more variants if needed\n",
    "from keras.applications import efficientnet_v2 as efficientnet_v2_preprocessing # Use alias\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    BatchNormalization,\n",
    "    # MaxPooling2D, # Keep if needed by custom modules, maybe not by main arch\n",
    "    Activation,\n",
    "    UpSampling2D,\n",
    "    concatenate,\n",
    "    Multiply,\n",
    "    GlobalAveragePooling2D,\n",
    "    Dense,\n",
    "    Reshape,\n",
    "    Add, # Import Add layer\n",
    ")\n",
    "from keras.losses import Loss\n",
    "from keras.metrics import Metric\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from tensorflow.keras import mixed_precision\n",
    "# from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix # Removed sklearn, use Keras metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61deead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit TensorFlow logs\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # 0 = all, 1 = INFO filtered, 2 = WARNING filtered, 3 = ERROR filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae839b",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f48523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Variant ---\n",
    "# Select the EfficientNetV2 variant by uncommenting ONE of the following lines:\n",
    "# EFFICIENTNET_VARIANT = 'EfficientNetV2B0'\n",
    "EFFICIENTNET_VARIANT = 'EfficientNetV2B1'\n",
    "# EFFICIENTNET_VARIANT = 'EfficientNetV2B2' # Example: add B2 if desired\n",
    "# EFFICIENTNET_VARIANT = 'EfficientNetV2B3' # Example: add B3 if desired\n",
    "\n",
    "# --- Input Dimensions based on Variant ---\n",
    "if EFFICIENTNET_VARIANT == 'EfficientNetV2B0':\n",
    "    IMG_HEIGHT = 224\n",
    "    IMG_WIDTH = 224\n",
    "elif EFFICIENTNET_VARIANT == 'EfficientNetV2B1':\n",
    "    IMG_HEIGHT = 240\n",
    "    IMG_WIDTH = 240\n",
    "elif EFFICIENTNET_VARIANT == 'EfficientNetV2B2':\n",
    "    IMG_HEIGHT = 260\n",
    "    IMG_WIDTH = 260\n",
    "elif EFFICIENTNET_VARIANT == 'EfficientNetV2B3':\n",
    "    IMG_HEIGHT = 300\n",
    "    IMG_WIDTH = 300\n",
    "else:\n",
    "    # Default or raise error for unsupported variants\n",
    "    print(f\"Warning: Using default size 224x224 for unlisted variant {EFFICIENTNET_VARIANT}\")\n",
    "    IMG_HEIGHT = 224\n",
    "    IMG_WIDTH = 224\n",
    "\n",
    "INPUT_CHANNELS = 3 # For EfficientNetV2 input\n",
    "\n",
    "# --- Data Loading Constants ---\n",
    "# Select modalities (Indices based on BraTS documentation: 0:T1, 1:T1ce, 2:T2, 3:FLAIR)\n",
    "MODALITY_INDICES = [1, 3]  # Use T1ce and FLAIR\n",
    "NUM_MODALITIES_LOADED = len(MODALITY_INDICES) # Should be 2\n",
    "# Define which loaded modality corresponds to which RGB channel for EfficientNetV2 input\n",
    "# Example: [T1ce, FLAIR, T1ce] -> Index 0, Index 1, Index 0 from the loaded modalities\n",
    "RGB_MAPPING_INDICES = [0, 1, 0] # Map T1ce to R and B, FLAIR to G\n",
    "\n",
    "# --- Training Constants ---\n",
    "# Adjust batch size based on GPU memory and image size\n",
    "BATCH_SIZE = 4      # Start low, adjust based on variant and GPU memory\n",
    "LEARNING_RATE = 1e-4 # Initial learning rate (Keep based on previous success)\n",
    "NUM_EPOCHS = 30     # Number of training epochs (or until early stopping)\n",
    "BUFFER_SIZE = 300   # Shuffle buffer size (adjust based on memory)\n",
    "THRESHOLD = 0.5     # Segmentation threshold for binary metrics and visualization\n",
    "\n",
    "# --- Loss Weights ---\n",
    "# Use weights from successful VGG16 run\n",
    "COMBINED_LOSS_WEIGHTS = {'bce_weight': 0.5, 'dice_weight': 0.5, 'class_weight': 100.0} # Tunable loss weights\n",
    "\n",
    "# --- Paths ---\n",
    "# Parameterize paths based on the EfficientNetV2 variant\n",
    "VARIANT_SUFFIX = EFFICIENTNET_VARIANT.lower() # e.g., \"efficientnetv2b0\"\n",
    "CHECKPOINT_DIR = f\"./{VARIANT_SUFFIX}-checkpoints\"\n",
    "CHECKPOINT_PATH = f\"{CHECKPOINT_DIR}/{VARIANT_SUFFIX}_as_net_model.weights.h5\"\n",
    "CHECKPOINT_BEST_PATH = f\"{CHECKPOINT_DIR}/{VARIANT_SUFFIX}_as_net_model_best.weights.h5\"\n",
    "OUTPUT_DIR = f\"{VARIANT_SUFFIX}-output\"\n",
    "COMPLETION_FILE = f\"{VARIANT_SUFFIX}-asnet-finished-training.txt\"\n",
    "DATASET_PATH = \"brats2020-training-data/\" # Make sure this points to the directory containing 'BraTS20 Training Metadata.csv' and 'content/data'\n",
    "METADATA_FILE = os.path.join(DATASET_PATH, \"BraTS20 Training Metadata.csv\")\n",
    "H5_DATA_DIR = os.path.join(DATASET_PATH, \"BraTS2020_training_data/content/data\") # Directory containing the .h5 slice files\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_DIR, \"examples\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0092d5c",
   "metadata": {},
   "source": [
    "## GPU Configuration and Mixed Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6454176",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"--- GPU Configuration ---\")\n",
    "# Configure memory growth to prevent GPU memory overflow upfront\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "        print(f\"Physical GPUs: {len(gpus)}, Logical GPUs: {len(logical_gpus)}\")\n",
    "\n",
    "        # MirroredStrategy for multi-GPU training\n",
    "        if len(gpus) > 1:\n",
    "             strategy = tf.distribute.MirroredStrategy()\n",
    "             print(f\"Running on {strategy.num_replicas_in_sync} GPU(s) using MirroredStrategy.\")\n",
    "        else:\n",
    "             strategy = tf.distribute.get_strategy() # Default for single GPU\n",
    "             print(\"Running on single GPU (default strategy).\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU Configuration Error: {e}. Falling back to default strategy.\")\n",
    "        strategy = tf.distribute.get_strategy() # Fallback to default (CPU or single GPU)\n",
    "        print(\"Running on CPU or single GPU (default strategy).\")\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # Default strategy (CPU)\n",
    "    print(\"No GPU detected. Running on CPU.\")\n",
    "print(\"Number of replicas in sync:\", strategy.num_replicas_in_sync)\n",
    "print(\"Global Batch Size (per replica * num replicas):\", BATCH_SIZE * strategy.num_replicas_in_sync)\n",
    "\n",
    "\n",
    "print(\"\\n--- Mixed Precision Configuration ---\")\n",
    "# Use mixed precision to reduce memory usage and potentially speed up training on compatible GPUs\n",
    "# policy = mixed_precision.Policy('mixed_float16') # Uncomment for mixed precision\n",
    "policy = mixed_precision.Policy('float32') # Using float32 as requested\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print(f\"Mixed precision policy set to: {policy.name}\")\n",
    "print('Compute dtype: %s' % policy.compute_dtype)\n",
    "print('Variable dtype: %s' % policy.variable_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45754016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure JIT compilation (Optional, can sometimes improve performance)\n",
    "# tf.config.optimizer.set_jit(True)\n",
    "# print(\"JIT compilation enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fad2c23",
   "metadata": {},
   "source": [
    "## Define AS-Net Model Architecture with EfficientNetV2 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015c315",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Helper function to find layer names (useful during development)\n",
    "def find_layer_names(model, substring):\n",
    "    names = []\n",
    "    for layer in model.layers:\n",
    "        if substring in layer.name:\n",
    "            names.append(layer.name)\n",
    "    return names\n",
    "\n",
    "# Add a custom layer for resizing features to match target dimensions\n",
    "from keras.layers import Layer\n",
    "\n",
    "class ResizeToMatchLayer(Layer):\n",
    "    \"\"\"Resizes input tensor to match the spatial dimensions of the target tensor.\"\"\"\n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        super(ResizeToMatchLayer, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # inputs[0] is the tensor to resize, inputs[1] is the target tensor\n",
    "        x_to_resize, target = inputs\n",
    "        target_shape = tf.shape(target)\n",
    "        target_height, target_width = target_shape[1], target_shape[2]\n",
    "        return tf.image.resize(x_to_resize, [target_height, target_width], method='bilinear')\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[1][1], input_shape[1][2], input_shape[0][3])\n",
    "\n",
    "\n",
    "def AS_Net(input_size=(IMG_HEIGHT, IMG_WIDTH, INPUT_CHANNELS), variant=EFFICIENTNET_VARIANT):\n",
    "    \"\"\"Defines the AS-Net model with an EfficientNetV2 encoder.\"\"\"\n",
    "    # Create an explicit input tensor with a name\n",
    "    inputs = Input(input_size, dtype=tf.float32, name='input_image')\n",
    "\n",
    "    # Preprocess input for EfficientNetV2 (typically 0 to 1 scaling, handled by the layer/function)\n",
    "    # The preprocessing function handles the specific scaling needs.\n",
    "    preprocessed_inputs = efficientnet_v2_preprocessing.preprocess_input(inputs)\n",
    "\n",
    "    # Load EfficientNetV2 backbone pre-trained on ImageNet\n",
    "    if variant == 'EfficientNetV2B0':\n",
    "        print(\"Loading EfficientNetV2B0 encoder...\")\n",
    "        base_model = EfficientNetV2B0(\n",
    "            weights=\"imagenet\",\n",
    "            include_top=False,\n",
    "            input_shape=input_size,\n",
    "            input_tensor=preprocessed_inputs\n",
    "        )\n",
    "        # Updated skip connection layers based on actual EfficientNetV2B0 architecture\n",
    "        skip_layer_names = [\n",
    "            'block1a_project_activation', # ~112x112 (for 224 input)\n",
    "            'block2b_add',                # ~56x56\n",
    "            'block3b_add',                # ~28x28\n",
    "            'block5e_add',                # ~14x14\n",
    "            'top_activation'              # ~7x7 (Bottleneck)\n",
    "        ]\n",
    "        fine_tune_start_block = 4 # Start unfreezing from block 4 or 5\n",
    "    elif variant == 'EfficientNetV2B1':\n",
    "        print(\"Loading EfficientNetV2B1 encoder...\")\n",
    "        base_model = EfficientNetV2B1(\n",
    "            weights=\"imagenet\",\n",
    "            include_top=False,\n",
    "            input_shape=input_size,\n",
    "            input_tensor=preprocessed_inputs\n",
    "        )\n",
    "        # Updated skip connection layers based on actual EfficientNetV2B1 architecture\n",
    "        skip_layer_names = [\n",
    "            'block1a_project_activation', # ~120x120 (for 240 input)\n",
    "            'block2c_add',                # ~60x60\n",
    "            'block3c_add',                # ~30x30\n",
    "            'block5f_add',                # ~15x15\n",
    "            'top_activation'              # ~8x8 (Bottleneck)\n",
    "        ]\n",
    "        fine_tune_start_block = 4 # Start unfreezing from block 4 or 5\n",
    "    elif variant == 'EfficientNetV2B2':\n",
    "        print(\"Loading EfficientNetV2B2 encoder...\")\n",
    "        base_model = EfficientNetV2B2(\n",
    "            weights=\"imagenet\",\n",
    "            include_top=False,\n",
    "            input_shape=input_size,\n",
    "            input_tensor=preprocessed_inputs\n",
    "        )\n",
    "        # Updated skip connection layers based on actual EfficientNetV2B2 architecture\n",
    "        skip_layer_names = [\n",
    "            'block1a_project_activation', # ~130x130 (for 260 input)\n",
    "            'block2c_add',                # ~65x65\n",
    "            'block3c_add',                # ~33x33\n",
    "            'block5g_add',                # ~17x17\n",
    "            'top_activation'              # ~9x9 (Bottleneck)\n",
    "        ]\n",
    "        fine_tune_start_block = 4\n",
    "    elif variant == 'EfficientNetV2B3':\n",
    "        print(\"Loading EfficientNetV2B3 encoder...\")\n",
    "        base_model = EfficientNetV2B3(\n",
    "            weights=\"imagenet\",\n",
    "            include_top=False,\n",
    "            input_shape=input_size,\n",
    "            input_tensor=preprocessed_inputs\n",
    "        )\n",
    "        # Updated skip connection layers based on actual EfficientNetV2B3 architecture\n",
    "        skip_layer_names = [\n",
    "            'block1a_project_activation', # ~150x150 (for 300 input)\n",
    "            'block2d_add',                # ~75x75\n",
    "            'block3d_add',                # ~38x38\n",
    "            'block5j_add',                # ~19x19\n",
    "            'top_activation'              # ~10x10 (Bottleneck)\n",
    "        ]\n",
    "        fine_tune_start_block = 4\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported EfficientNetV2 variant: {variant}\")\n",
    "\n",
    "    # --- Fine-tuning ---\n",
    "    base_model.trainable = True # Ensure base is trainable before selective freezing\n",
    "    freeze_until_block = f'block{fine_tune_start_block}'\n",
    "    print(f\"Fine-tuning: Freezing layers up to block '{freeze_until_block}'...\")\n",
    "    layer_found = False\n",
    "    for layer in base_model.layers:\n",
    "        if freeze_until_block in layer.name:\n",
    "            layer_found = True\n",
    "            print(f\"Reached block '{freeze_until_block}' at layer: {layer.name}. Unfreezing subsequent layers.\")\n",
    "        if not layer_found:\n",
    "            layer.trainable = False\n",
    "        else:\n",
    "            # Ensure Batch Norm layers remain frozen if desired (often recommended during fine-tuning)\n",
    "            # Or keep them trainable if BN stats need adjustment for medical images\n",
    "            # if isinstance(layer, BatchNormalization):\n",
    "            #     layer.trainable = False\n",
    "            # else:\n",
    "            layer.trainable = True # Unfreeze layers from the target block onwards\n",
    "\n",
    "    if not layer_found:\n",
    "        print(f\"Warning: Block '{freeze_until_block}' not found for freezing. Fine-tuning all layers.\")\n",
    "        for layer in base_model.layers: # Ensure all are trainable if block wasn't found\n",
    "             layer.trainable = True\n",
    "\n",
    "    # Count trainable/non-trainable params\n",
    "    trainable_count = sum([tf.keras.backend.count_params(w) for w in base_model.trainable_weights])\n",
    "    non_trainable_count = sum([tf.keras.backend.count_params(w) for w in base_model.non_trainable_weights])\n",
    "    print(f'Base Model - Trainable params: {trainable_count:,} | Non-trainable params: {non_trainable_count:,}')\n",
    "    # --- End Fine-tuning ---\n",
    "\n",
    "\n",
    "    # Extract feature maps from EfficientNetV2 encoder stages\n",
    "    encoder_outputs = []\n",
    "    print(\"Extracting skip connections from layers:\")\n",
    "    for name in skip_layer_names:\n",
    "        try:\n",
    "            layer_output = base_model.get_layer(name).output\n",
    "            encoder_outputs.append(layer_output)\n",
    "            print(f\" - {name}: Shape {layer_output.shape}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"\\nERROR: Could not find layer '{name}' in {variant}. Error: {e}\")\n",
    "            print(\"Available layers (first 50):\")\n",
    "            for i, layer in enumerate(base_model.layers):\n",
    "                 # Fix the error for printing layer information\n",
    "                 if hasattr(layer, 'output_shape'):\n",
    "                     print(f\"  {i}: {layer.name} - Output Shape: {layer.output_shape}\")\n",
    "                 else:\n",
    "                     print(f\"  {i}: {layer.name}\")\n",
    "                 if i > 50: break # Print only first few layers\n",
    "            raise ValueError(f\"Layer {name} not found. Check skip_layer_names for {variant}.\")\n",
    "\n",
    "    # Unpack encoder outputs (adjust based on the number of skip connections)\n",
    "    if len(encoder_outputs) != 5:\n",
    "        raise ValueError(f\"Expected 5 skip connections, but got {len(encoder_outputs)} for {variant}\")\n",
    "    output1, output2, output3, output4, bottleneck = encoder_outputs\n",
    "    # Shapes for B0: (112, 112), (56, 56), (28, 28), (14, 14), (7, 7)\n",
    "    # Shapes for B1: (120, 120), (60, 60), (30, 30), (15, 15), (8, 8)\n",
    "    # Shapes for B2: (130, 130), (65, 65), (33, 33), (17, 17), (9, 9)\n",
    "    # Shapes for B3: (150, 150), (75, 75), (38, 38), (19, 19), (10, 10)\n",
    "\n",
    "\n",
    "    # --- Decoder with SAM, CAM, and Synergy ---\n",
    "    # Adapts the VGG16/MobileNetV3 decoder structure\n",
    "    # Handles potential odd dimensions from some EfficientNet variants via bilinear upsampling\n",
    "\n",
    "    # Decoder Stage 1: Bottleneck (H/32) -> H/16\n",
    "    print(f\"\\nDecoder Stage 1 (H/32 -> H/16)\")\n",
    "    up4 = UpSampling2D(size=(2, 2), interpolation=\"bilinear\", name='up_bottleneck')(bottleneck)\n",
    "    print(f\"  Upsampled bottleneck shape: {up4.shape}\")\n",
    "    print(f\"  Target skip connection (output4) shape: {output4.shape}\")\n",
    "    \n",
    "    # Use custom layer to resize up4 to match output4\n",
    "    up4 = ResizeToMatchLayer(name='resize_up4')([up4, output4])\n",
    "    print(f\"  Resized upsampled bottleneck shape: {up4.shape}\")\n",
    "    \n",
    "    merge4 = concatenate([output4, up4], axis=-1, name='merge4')\n",
    "    filters4 = merge4.shape[-1]\n",
    "    print(f\"  Concatenated shape (H/16): {merge4.shape}, Filters: {filters4}\")\n",
    "    SAM4 = SAM(filters=filters4, name='sam4')(merge4)\n",
    "    CAM4 = CAM(filters=filters4, name='cam4')(merge4)\n",
    "    print(f\"  SAM4/CAM4 output shape: {SAM4.shape}\")\n",
    "\n",
    "    # Decoder Stage 2: H/16 -> H/8\n",
    "    print(f\"Decoder Stage 2 (H/16 -> H/8)\")\n",
    "    up_sam4 = UpSampling2D((2, 2), interpolation=\"bilinear\", name='up_sam4')(SAM4)\n",
    "    up_cam4 = UpSampling2D((2, 2), interpolation=\"bilinear\", name='up_cam4')(CAM4)\n",
    "    print(f\"  Upsampled SAM4/CAM4 shape: {up_sam4.shape}\")\n",
    "    print(f\"  Target skip connection (output3) shape: {output3.shape}\")\n",
    "    \n",
    "    # Use custom layer to resize upsampled features\n",
    "    up_sam4 = ResizeToMatchLayer(name='resize_up_sam4')([up_sam4, output3])\n",
    "    up_cam4 = ResizeToMatchLayer(name='resize_up_cam4')([up_cam4, output3])\n",
    "    \n",
    "    merge31 = concatenate([output3, up_sam4], axis=-1, name='merge31')\n",
    "    merge32 = concatenate([output3, up_cam4], axis=-1, name='merge32')\n",
    "    filters3 = merge31.shape[-1]\n",
    "    print(f\"  Concatenated shape (H/8): {merge31.shape}, Filters: {filters3}\")\n",
    "    SAM3 = SAM(filters=filters3, name='sam3')(merge31)\n",
    "    CAM3 = CAM(filters=filters3, name='cam3')(merge32)\n",
    "    print(f\"  SAM3/CAM3 output shape: {SAM3.shape}\")\n",
    "\n",
    "    # Decoder Stage 3: H/8 -> H/4\n",
    "    print(f\"Decoder Stage 3 (H/8 -> H/4)\")\n",
    "    up_sam3 = UpSampling2D((2, 2), interpolation=\"bilinear\", name='up_sam3')(SAM3)\n",
    "    up_cam3 = UpSampling2D((2, 2), interpolation=\"bilinear\", name='up_cam3')(CAM3)\n",
    "    print(f\"  Upsampled SAM3/CAM3 shape: {up_sam3.shape}\")\n",
    "    print(f\"  Target skip connection (output2) shape: {output2.shape}\")\n",
    "    \n",
    "    # Use custom layer to resize upsampled features\n",
    "    up_sam3 = ResizeToMatchLayer(name='resize_up_sam3')([up_sam3, output2])\n",
    "    up_cam3 = ResizeToMatchLayer(name='resize_up_cam3')([up_cam3, output2])\n",
    "    \n",
    "    merge21 = concatenate([output2, up_sam3], axis=-1, name='merge21')\n",
    "    merge22 = concatenate([output2, up_cam3], axis=-1, name='merge22')\n",
    "    filters2 = merge21.shape[-1]\n",
    "    print(f\"  Concatenated shape (H/4): {merge21.shape}, Filters: {filters2}\")\n",
    "    SAM2 = SAM(filters=filters2, name='sam2')(merge21)\n",
    "    CAM2 = CAM(filters=filters2, name='cam2')(merge22)\n",
    "    print(f\"  SAM2/CAM2 output shape: {SAM2.shape}\")\n",
    "\n",
    "    # Decoder Stage 4: H/4 -> H/2\n",
    "    print(f\"Decoder Stage 4 (H/4 -> H/2)\")\n",
    "    up_sam2 = UpSampling2D((2, 2), interpolation=\"bilinear\", name='up_sam2')(SAM2)\n",
    "    up_cam2 = UpSampling2D((2, 2), interpolation=\"bilinear\", name='up_cam2')(CAM2)\n",
    "    print(f\"  Upsampled SAM2/CAM2 shape: {up_sam2.shape}\")\n",
    "    print(f\"  Target skip connection (output1) shape: {output1.shape}\")\n",
    "    \n",
    "    # Use custom layer to resize upsampled features\n",
    "    up_sam2 = ResizeToMatchLayer(name='resize_up_sam2')([up_sam2, output1])\n",
    "    up_cam2 = ResizeToMatchLayer(name='resize_up_cam2')([up_cam2, output1])\n",
    "    \n",
    "    merge11 = concatenate([output1, up_sam2], axis=-1, name='merge11')\n",
    "    merge12 = concatenate([output1, up_cam2], axis=-1, name='merge12')\n",
    "    filters1 = merge11.shape[-1]\n",
    "    print(f\"  Concatenated shape (H/2): {merge11.shape}, Filters: {filters1}\")\n",
    "    SAM1 = SAM(filters=filters1, name='sam1')(merge11)\n",
    "    CAM1 = CAM(filters=filters1, name='cam1')(merge12)\n",
    "    print(f\"  SAM1/CAM1 output shape: {SAM1.shape}\")\n",
    "\n",
    "    # Final Upsampling Stage: H/2 -> H (Full Resolution)\n",
    "    print(f\"Decoder Stage 5 (H/2 -> H)\")\n",
    "    final_up_sam = UpSampling2D((2, 2), interpolation=\"bilinear\", name='final_up_sam')(SAM1)\n",
    "    final_up_cam = UpSampling2D((2, 2), interpolation=\"bilinear\", name='final_up_cam')(CAM1)\n",
    "    print(f\"  Final upsampled SAM/CAM shape: {final_up_sam.shape}\")\n",
    "\n",
    "\n",
    "    # Synergy module to combine final SAM and CAM outputs\n",
    "    synergy_output = Synergy(name='synergy')([final_up_sam, final_up_cam]) # Input shapes: (H, W, C_out_synergy) -> Output: (H, W, 1)\n",
    "    print(f\"  Synergy output shape: {synergy_output.shape}\")\n",
    "\n",
    "    # Final 1x1 convolution for segmentation map (use float32 for stability)\n",
    "    output = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\", name='final_output', dtype='float32')(synergy_output)\n",
    "    print(f\"  Final output shape: {output.shape}\")\n",
    "\n",
    "    # Create the model with explicit input and output\n",
    "    model = Model(inputs=inputs, outputs=output, name=f'AS_Net_{variant}')\n",
    "\n",
    "    # Clean up memory\n",
    "    gc.collect()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- SAM Module ---\n",
    "# (Keep SAM, CAM, Synergy classes exactly as they were in the VGG16/MobileNetV3 version)\n",
    "class SAM(Model):\n",
    "    \"\"\"Spatial Attention Module\"\"\"\n",
    "    def __init__(self, filters, name='sam', **kwargs):\n",
    "        super(SAM, self).__init__(name=name, **kwargs)\n",
    "        self.filters = filters\n",
    "        # Output channels reduction (ensure reasonable number of channels)\n",
    "        self.out_channels = max(16, filters // 8 if filters > 128 else filters // 4) # Adjust reduction based on input filters\n",
    "\n",
    "        # Convolution layers using compute dtype\n",
    "        compute_dtype = mixed_precision.global_policy().compute_dtype\n",
    "        self.conv1 = Conv2D(self.out_channels, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv1')\n",
    "        self.conv2 = Conv2D(self.out_channels, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv2')\n",
    "        self.conv3 = Conv2D(self.out_channels, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv3') # F'(X) path\n",
    "        self.conv4 = Conv2D(self.out_channels, 1, activation=\"relu\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv4') # F''(X) path (shortcut)\n",
    "\n",
    "        # Pooling and Upsampling for spatial attention map generation\n",
    "        # Use MaxPooling2D from Keras layers\n",
    "        from keras.layers import MaxPooling2D\n",
    "        self.pool1 = MaxPooling2D((2, 2), name='pool1')\n",
    "        self.upsample1 = UpSampling2D((2, 2), interpolation=\"bilinear\", name='upsample1')\n",
    "        self.W1 = Conv2D(1, 1, activation=\"sigmoid\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='W1') # Output 1 channel attention map\n",
    "\n",
    "        self.pool2 = MaxPooling2D((4, 4), name='pool2')\n",
    "        self.upsample2 = UpSampling2D((4, 4), interpolation=\"bilinear\", name='upsample2')\n",
    "        self.W2 = Conv2D(1, 1, activation=\"sigmoid\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='W2') # Output 1 channel attention map\n",
    "\n",
    "        self.add_attention = Add(name='add_attention')\n",
    "        self.multiply = Multiply(name='multiply_attention')\n",
    "        self.add_residual = Add(name='add_residual')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out1 = self.conv3(self.conv2(self.conv1(inputs))) # Main feature path F'(X)\n",
    "        out2 = self.conv4(inputs) # Shortcut path F''(X)\n",
    "\n",
    "        # Parallel attention branches (on shortcut path F''(X))\n",
    "        pool1 = self.pool1(out2)\n",
    "        up1 = self.upsample1(pool1)\n",
    "        # Explicit resize to match shortcut path dimension precisely if needed (e.g., due to padding)\n",
    "        up1 = tf.image.resize(up1, size=tf.shape(out2)[1:3], method=\"bilinear\")\n",
    "        att1 = self.W1(up1) # S1 (shape: B, H, W, 1)\n",
    "\n",
    "        pool2 = self.pool2(out2)\n",
    "        up2 = self.upsample2(pool2)\n",
    "        up2 = tf.image.resize(up2, size=tf.shape(out2)[1:3], method=\"bilinear\")\n",
    "        att2 = self.W2(up2) # S2 (shape: B, H, W, 1)\n",
    "\n",
    "        # Combine attention weights (S)\n",
    "        attention_map = self.add_attention([att1, att2]) # S = S1 + S2 (shape: B, H, W, 1)\n",
    "\n",
    "        # Apply attention: Multiply main path by attention weights and add shortcut\n",
    "        # Y = F'(X) * S + F''(X)\n",
    "        # Attention map (B,H,W,1) is broadcasted across channels of out1 (B,H,W,C_out)\n",
    "        attended_features = self.multiply([out1, attention_map]) # F'(X) * S\n",
    "        y = self.add_residual([attended_features, out2]) # Add shortcut F''(X)\n",
    "        return y\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SAM, self).get_config()\n",
    "        config.update({\"filters\": self.filters})\n",
    "        return config\n",
    "\n",
    "# --- CAM Module ---\n",
    "class CAM(Model):\n",
    "    \"\"\"Channel Attention Module\"\"\"\n",
    "    def __init__(self, filters, reduction_ratio=16, name='cam', **kwargs):\n",
    "        super(CAM, self).__init__(name=name, **kwargs)\n",
    "        self.filters = filters\n",
    "        # Output channels reduction (ensure reasonable number of channels)\n",
    "        self.out_channels = max(16, filters // 8 if filters > 128 else filters // 4) # Adjust reduction based on input filters\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "\n",
    "        # Convolution layers using compute dtype\n",
    "        compute_dtype = mixed_precision.global_policy().compute_dtype\n",
    "        self.conv1 = Conv2D(self.out_channels, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv1')\n",
    "        self.conv2 = Conv2D(self.out_channels, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv2')\n",
    "        self.conv3 = Conv2D(self.out_channels, 3, activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv3') # F'(X) path\n",
    "        self.conv4 = Conv2D(self.out_channels, 1, activation=\"relu\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv4') # F''(X) path (shortcut)\n",
    "\n",
    "        # Channel attention mechanism\n",
    "        self.gpool = GlobalAveragePooling2D(name='global_avg_pool', keepdims=True) # Keep dims for broadcasting\n",
    "        reduced_channels = max(1, self.out_channels // self.reduction_ratio) # Ensure at least 1 channel\n",
    "        self.fc1 = Dense(reduced_channels, activation=\"relu\", use_bias=False, dtype=compute_dtype, name='fc1')\n",
    "        self.fc2 = Dense(self.out_channels, activation=\"sigmoid\", use_bias=False, dtype=compute_dtype, name='fc2')\n",
    "\n",
    "        self.multiply = Multiply(name='multiply_attention')\n",
    "        self.add_residual = Add(name='add_residual')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        out1 = self.conv3(self.conv2(self.conv1(inputs))) # Main feature path F'(X)\n",
    "        out2 = self.conv4(inputs) # Shortcut path F''(X)\n",
    "\n",
    "        # Calculate channel attention weights (C) from shortcut path F''(X)\n",
    "        pooled = self.gpool(out2) # Global Average Pooling -> (Batch, 1, 1, C_out)\n",
    "        fc1_out = self.fc1(pooled)\n",
    "        channel_attention_weights = self.fc2(fc1_out) # Shape: (Batch, 1, 1, C_out)\n",
    "\n",
    "        # Apply attention: Multiply main path by channel attention weights and add shortcut\n",
    "        # Y = F'(X) * C + F''(X)\n",
    "        recalibrated_features = self.multiply([out1, channel_attention_weights]) # F'(X) * C\n",
    "        y = self.add_residual([recalibrated_features, out2]) # Add shortcut F''(X)\n",
    "        return y\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CAM, self).get_config()\n",
    "        config.update({\"filters\": self.filters, \"reduction_ratio\": self.reduction_ratio})\n",
    "        return config\n",
    "\n",
    "# --- Synergy Module ---\n",
    "class Synergy(Model):\n",
    "    \"\"\"Combines SAM and CAM outputs with learnable weights.\"\"\"\n",
    "    def __init__(self, alpha_init=0.5, beta_init=0.5, name='synergy', **kwargs):\n",
    "        super(Synergy, self).__init__(name=name, **kwargs)\n",
    "        # Use tf.Variable for learnable weights, ensure they are float32 for stability\n",
    "        self.alpha = tf.Variable(alpha_init, trainable=True, name=\"alpha\", dtype=tf.float32)\n",
    "        self.beta = tf.Variable(beta_init, trainable=True, name=\"beta\", dtype=tf.float32)\n",
    "\n",
    "        # Conv + BN after weighted sum. Output should have 1 channel for final segmentation.\n",
    "        compute_dtype = mixed_precision.global_policy().compute_dtype\n",
    "        # Ensure output is 1 channel for the final sigmoid in AS_Net\n",
    "        self.conv = Conv2D(1, 1, padding=\"same\", kernel_initializer=\"he_normal\", dtype=compute_dtype, name='conv')\n",
    "        self.bn = BatchNormalization(name='bn')\n",
    "        self.add = Add(name='add_weighted') # Use Add layer\n",
    "\n",
    "    def call(self, inputs):\n",
    "        sam_features, cam_features = inputs # Expecting tuple/list: (SAM_output, CAM_output)\n",
    "\n",
    "        # Cast learnable weights to the compute dtype of the inputs for multiplication\n",
    "        compute_dtype = sam_features.dtype\n",
    "        alpha_casted = tf.cast(self.alpha, compute_dtype)\n",
    "        beta_casted = tf.cast(self.beta, compute_dtype)\n",
    "\n",
    "        # Weighted sum: alpha * SAM_output + beta * CAM_output\n",
    "        weighted_sum = self.add([alpha_casted * sam_features, beta_casted * cam_features])\n",
    "\n",
    "        # Apply Conv -> BN\n",
    "        convolved = self.conv(weighted_sum)\n",
    "        bn_out = self.bn(convolved)\n",
    "        # No activation here, final sigmoid is in the main model output layer\n",
    "        return bn_out # Output shape: (B, H, W, 1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Synergy, self).get_config()\n",
    "        # Store initial values, actual values are saved in weights\n",
    "        config.update({\"alpha_init\": 0.5, \"beta_init\": 0.5})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789abcdd",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7c4ee",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Keep Loss functions (DiceLoss, WBCE, CombinedLoss) exactly as they were\n",
    "# Ensure they handle potential dtype casting correctly if using mixed precision\n",
    "\n",
    "class DiceLoss(Loss):\n",
    "    \"\"\"Computes the Dice Loss.\"\"\"\n",
    "    def __init__(self, smooth=1e-6, name='dice_loss', **kwargs):\n",
    "        super(DiceLoss, self).__init__(name=name, reduction='sum_over_batch_size', **kwargs)\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        y_true_f = tf.reshape(y_true, [-1])\n",
    "        y_pred_f = tf.reshape(y_pred, [-1])\n",
    "        intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "        union = tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f)\n",
    "        dice_coef = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1.0 - dice_coef\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DiceLoss, self).get_config()\n",
    "        config.update({\"smooth\": self.smooth})\n",
    "        return config\n",
    "\n",
    "\n",
    "class WBCE(Loss):\n",
    "    \"\"\"Weighted Binary Cross-Entropy Loss.\"\"\"\n",
    "    def __init__(self, weight=1.0, name='weighted_bce_loss', **kwargs):\n",
    "        super(WBCE, self).__init__(name=name, reduction='sum_over_batch_size', **kwargs)\n",
    "        self.weight = tf.cast(weight, tf.float32) # Store weight as float32\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, y_pred.dtype) # Match prediction dtype\n",
    "        epsilon_ = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon_, 1.0 - epsilon_)\n",
    "        logits = tf.math.log(y_pred / (1.0 - y_pred))\n",
    "        loss = tf.nn.weighted_cross_entropy_with_logits(\n",
    "            labels=y_true,\n",
    "            logits=logits,\n",
    "            pos_weight=tf.cast(self.weight, logits.dtype) # Cast weight to logits dtype\n",
    "        )\n",
    "        # Framework handles reduction based on 'sum_over_batch_size'\n",
    "        return loss\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(WBCE, self).get_config()\n",
    "        config.update({\"weight\": self.weight.numpy()}) # Store numpy value\n",
    "        return config\n",
    "\n",
    "# Combined Loss (Dice + Weighted BCE)\n",
    "class CombinedLoss(Loss):\n",
    "    \"\"\"Combines Dice Loss and Weighted Binary Cross-Entropy.\"\"\"\n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5, class_weight=1.0, name='combined_loss', **kwargs):\n",
    "        super(CombinedLoss, self).__init__(name=name, reduction='sum_over_batch_size', **kwargs)\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.wbce = WBCE(weight=class_weight)\n",
    "        self.dice_loss = DiceLoss()\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, y_pred.dtype)\n",
    "        bce_loss_val = self.wbce(y_true, y_pred)\n",
    "        dice_loss_val = self.dice_loss(y_true, y_pred)\n",
    "        # Combine losses with weights; framework handles reduction\n",
    "        combined = (self.bce_weight * bce_loss_val) + (self.dice_weight * dice_loss_val)\n",
    "        return combined\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CombinedLoss, self).get_config()\n",
    "        config.update({\n",
    "            \"bce_weight\": self.bce_weight,\n",
    "            \"dice_weight\": self.dice_weight,\n",
    "            \"class_weight\": self.wbce.weight.numpy()\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f61ec1",
   "metadata": {},
   "source": [
    "## Custom Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad46a2fa",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Keep Metrics (DiceCoefficient, IoU) exactly as they were\n",
    "# Ensure they handle potential dtype casting and use tf operations\n",
    "\n",
    "# Dice Coefficient Metric\n",
    "class DiceCoefficient(Metric):\n",
    "    \"\"\"Computes the Dice Coefficient metric.\"\"\"\n",
    "    def __init__(self, threshold=THRESHOLD, smooth=1e-6, name='dice_coefficient', dtype=None):\n",
    "        super(DiceCoefficient, self).__init__(name=name, dtype=dtype)\n",
    "        self.threshold = threshold\n",
    "        self.smooth = smooth\n",
    "        self.intersection_sum = self.add_weight(name='intersection_sum', initializer='zeros', dtype=tf.float32)\n",
    "        self.union_sum = self.add_weight(name='union_sum', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred_binary = tf.cast(y_pred >= self.threshold, tf.float32)\n",
    "        y_true_f = tf.reshape(y_true, [-1])\n",
    "        y_pred_f = tf.reshape(y_pred_binary, [-1])\n",
    "        intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "        pred_sum = tf.reduce_sum(y_pred_f)\n",
    "        true_sum = tf.reduce_sum(y_true_f)\n",
    "        self.intersection_sum.assign_add(intersection)\n",
    "        self.union_sum.assign_add(true_sum + pred_sum)\n",
    "\n",
    "    def result(self):\n",
    "        dice = (2.0 * self.intersection_sum + self.smooth) / (self.union_sum + self.smooth)\n",
    "        return tf.cast(dice, self._dtype) if self._dtype else dice\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.intersection_sum.assign(0.0)\n",
    "        self.union_sum.assign(0.0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(DiceCoefficient, self).get_config()\n",
    "        config.update({\"threshold\": self.threshold, \"smooth\": self.smooth})\n",
    "        return config\n",
    "\n",
    "\n",
    "# IoU Metric (Jaccard)\n",
    "class IoU(Metric):\n",
    "    \"\"\"Computes the Intersection over Union (IoU) or Jaccard Index.\"\"\"\n",
    "    def __init__(self, threshold=THRESHOLD, smooth=1e-6, name='iou', dtype=None):\n",
    "        super(IoU, self).__init__(name=name, dtype=dtype)\n",
    "        self.threshold = threshold\n",
    "        self.smooth = smooth\n",
    "        self.intersection_sum = self.add_weight(name='intersection_sum', initializer='zeros', dtype=tf.float32)\n",
    "        self.union_sum = self.add_weight(name='union_sum', initializer='zeros', dtype=tf.float32)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        y_pred_binary = tf.cast(y_pred >= self.threshold, tf.float32)\n",
    "        y_true_f = tf.reshape(y_true, [-1])\n",
    "        y_pred_f = tf.reshape(y_pred_binary, [-1])\n",
    "        intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "        true_sum = tf.reduce_sum(y_true_f)\n",
    "        pred_sum = tf.reduce_sum(y_pred_f)\n",
    "        union = true_sum + pred_sum - intersection\n",
    "        self.intersection_sum.assign_add(intersection)\n",
    "        self.union_sum.assign_add(union)\n",
    "\n",
    "    def result(self):\n",
    "        iou = (self.intersection_sum + self.smooth) / (self.union_sum + self.smooth)\n",
    "        return tf.cast(iou, self._dtype) if self._dtype else iou\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.intersection_sum.assign(0.0)\n",
    "        self.union_sum.assign(0.0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(IoU, self).get_config()\n",
    "        config.update({\"threshold\": self.threshold, \"smooth\": self.smooth})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be61344",
   "metadata": {},
   "source": [
    "## Data Preparation for BraTS (Generic - Preprocessing done in model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27652f0",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_brats_data_gpu(\n",
    "    metadata_file=METADATA_FILE,\n",
    "    h5_dir=H5_DATA_DIR,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH), # Use updated target size from constants\n",
    "    batch_size=BATCH_SIZE,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    modality_indices=MODALITY_INDICES,\n",
    "    rgb_mapping_indices=RGB_MAPPING_INDICES,\n",
    "    num_modalities_loaded=NUM_MODALITIES_LOADED,\n",
    "    input_channels=INPUT_CHANNELS, # Should be 3\n",
    "    validation_split=0.2,\n",
    "    random_seed=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepares the BraTS 2020 dataset from H5 slices using tf.data.\n",
    "    Outputs Z-scored RGB images; model-specific preprocessing happens in the model.\n",
    "    \"\"\"\n",
    "    print(\"--- Setting up Data Pipeline ---\")\n",
    "    print(f\"Target image size: {target_size}\")\n",
    "    # ... (rest of the path checking and metadata loading is the same)\n",
    "    if not os.path.exists(metadata_file):\n",
    "        raise FileNotFoundError(f\"Metadata file not found at {metadata_file}\")\n",
    "    if not os.path.exists(h5_dir):\n",
    "        raise FileNotFoundError(f\"H5 data directory not found at {h5_dir}\")\n",
    "\n",
    "    df = pd.read_csv(metadata_file)\n",
    "    df['full_path'] = df['slice_path'].apply(lambda x: os.path.join(h5_dir, os.path.basename(x)))\n",
    "    df = df[df['full_path'].apply(os.path.exists)].copy()\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No valid H5 files found based on metadata in {h5_dir}. Check paths.\")\n",
    "    print(f\"Found {len(df)} existing H5 files referenced in metadata.\")\n",
    "\n",
    "    df = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)\n",
    "    split_idx = int(len(df) * (1.0 - validation_split))\n",
    "    train_df = df.iloc[:split_idx]\n",
    "    val_df = df.iloc[split_idx:]\n",
    "    print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
    "\n",
    "    train_files = train_df[\"full_path\"].tolist()\n",
    "    val_files = val_df[\"full_path\"].tolist()\n",
    "\n",
    "    # Function to parse H5 file (remains the same)\n",
    "    def parse_h5_file(file_path):\n",
    "        def _parse_h5(path_tensor):\n",
    "            path = path_tensor.numpy().decode(\"utf-8\")\n",
    "            try:\n",
    "                with h5py.File(path, \"r\") as hf:\n",
    "                    image_data = hf[\"image\"][()] # (H, W, 4) float64\n",
    "                    mask_data = hf[\"mask\"][()]   # (H, W, 3) uint8\n",
    "                    selected_modalities = image_data[:, :, modality_indices].astype(np.float32)\n",
    "                    binary_mask = np.logical_or.reduce((mask_data[:, :, 0] > 0,\n",
    "                                                        mask_data[:, :, 1] > 0,\n",
    "                                                        mask_data[:, :, 2] > 0)).astype(np.float32)\n",
    "                    return selected_modalities, binary_mask\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {path}: {e}\")\n",
    "                original_h, original_w = 240, 240\n",
    "                return (np.zeros((original_h, original_w, num_modalities_loaded), dtype=np.float32),\n",
    "                        np.zeros((original_h, original_w), dtype=np.float32))\n",
    "\n",
    "        image, mask = tf.py_function(_parse_h5, [file_path], [tf.float32, tf.float32])\n",
    "        original_h, original_w = 240, 240 # Assume original BraTS size\n",
    "        image.set_shape([original_h, original_w, num_modalities_loaded])\n",
    "        mask.set_shape([original_h, original_w])\n",
    "        return image, mask\n",
    "\n",
    "    # Function to preprocess: Z-score, RGB map, Resize, Finalize Mask\n",
    "    def preprocess(image, mask):\n",
    "        # --- Normalization (Z-score per modality) ---\n",
    "        normalized_channels = []\n",
    "        for i in range(num_modalities_loaded):\n",
    "            channel = image[:, :, i]\n",
    "            mean = tf.reduce_mean(channel)\n",
    "            std = tf.math.reduce_std(channel)\n",
    "            normalized_channel = (channel - mean) / (std + 1e-8) # Add epsilon\n",
    "            normalized_channels.append(normalized_channel)\n",
    "        image_normalized = tf.stack(normalized_channels, axis=-1)\n",
    "\n",
    "        # --- RGB Mapping ---\n",
    "        rgb_channels = [image_normalized[:, :, idx] for idx in rgb_mapping_indices]\n",
    "        image_rgb = tf.stack(rgb_channels, axis=-1) # Shape: (H_orig, W_orig, 3)\n",
    "\n",
    "        # --- Resizing ---\n",
    "        image_resized = tf.image.resize(image_rgb, target_size, method='bilinear')\n",
    "        mask_expanded = tf.expand_dims(mask, axis=-1)\n",
    "        mask_resized = tf.image.resize(mask_expanded, target_size, method='nearest')\n",
    "        mask_final = tf.squeeze(mask_resized, axis=-1)\n",
    "        mask_final = tf.cast(mask_final > 0.5, tf.float32) # Ensure binary\n",
    "        mask_final = tf.expand_dims(mask_final, axis=-1) # Add channel dim (H, W, 1)\n",
    "\n",
    "        # --- Final Image Format ---\n",
    "        # Ensure image is float32. Model-specific preprocessing (like EfficientNetV2's)\n",
    "        # will be applied INSIDE the model definition.\n",
    "        image_final = tf.cast(image_resized, tf.float32)\n",
    "\n",
    "        # --- Set Final Shapes ---\n",
    "        image_final.set_shape([target_size[0], target_size[1], input_channels])\n",
    "        mask_final.set_shape([target_size[0], target_size[1], 1])\n",
    "\n",
    "        return image_final, mask_final\n",
    "\n",
    "    # --- Create tf.data Datasets ---\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(val_files)\n",
    "\n",
    "    # --- Apply Transformations ---\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "\n",
    "    train_dataset = (\n",
    "        train_dataset.with_options(options)\n",
    "        .shuffle(buffer_size) # Shuffle file paths\n",
    "        .map(parse_h5_file, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    val_dataset = (\n",
    "        val_dataset.with_options(options)\n",
    "        # No shuffle for validation\n",
    "        .map(parse_h5_file, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    print(\"Data pipeline created successfully.\")\n",
    "    print(\"Train Dataset Element Spec:\", train_dataset.element_spec)\n",
    "    print(\"Validation Dataset Element Spec:\", val_dataset.element_spec)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# Function to visualize samples from the dataset (shows Z-scored input)\n",
    "def visualize_dataset_samples(dataset, num_samples=3, output_dir=OUTPUT_DIR):\n",
    "    \"\"\"Visualizes samples (Z-scored inputs) from the dataset and saves the plot.\"\"\"\n",
    "    print(\"--- Visualizing Dataset Samples ---\")\n",
    "    try:\n",
    "        plt.figure(figsize=(15, 5 * num_samples))\n",
    "        plot_count = 0\n",
    "        for images, masks in dataset.take(1): # Take one batch\n",
    "            num_in_batch = images.shape[0]\n",
    "            print(f\"Batch shapes - Images: {images.shape}, Masks: {masks.shape}\")\n",
    "            print(f\"Image dtype: {images.dtype}, Mask dtype: {masks.dtype}\")\n",
    "            print(f\"Image value range (Z-scored): Min={tf.reduce_min(images):.4f}, Max={tf.reduce_max(images):.4f}\")\n",
    "            print(f\"Mask value range: Min={tf.reduce_min(masks):.4f}, Max={tf.reduce_max(masks):.4f}\")\n",
    "\n",
    "            for i in range(min(num_samples, num_in_batch)):\n",
    "                plot_count += 1\n",
    "                img_zscored = images[i].numpy()\n",
    "                mask = masks[i].numpy().squeeze()\n",
    "\n",
    "                titles = [\"Input R (Z)\", \"Input G (Z)\", \"Input B (Z)\", \"RGB Input (Z-scored)\", \"Ground Truth Mask\"]\n",
    "                channels_to_plot = [img_zscored[:, :, 0], img_zscored[:, :, 1], img_zscored[:, :, 2], img_zscored, mask]\n",
    "\n",
    "                for j, item in enumerate(channels_to_plot):\n",
    "                    ax = plt.subplot(num_samples, len(titles), i * len(titles) + j + 1)\n",
    "                    ax.set_title(titles[j])\n",
    "\n",
    "                    if j < 4: # Z-scored image data\n",
    "                        # Clip Z-score data approx to [0,1] for display using percentiles\n",
    "                        p_low, p_high = np.percentile(item.flatten(), [2, 98])\n",
    "                        item_disp = np.clip((item - p_low) / (p_high - p_low + 1e-8), 0, 1)\n",
    "                        if np.isnan(item_disp).any() or np.isinf(item_disp).any(): item_disp = np.nan_to_num(item_disp)\n",
    "                        plt.imshow(item_disp, cmap='gray' if item.ndim==2 else None)\n",
    "                    else: # Mask\n",
    "                        if np.isnan(item).any() or np.isinf(item).any(): item = np.nan_to_num(item)\n",
    "                        plt.imshow(item, cmap='jet', vmin=0, vmax=1)\n",
    "\n",
    "                    plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout(pad=0.5)\n",
    "        save_path = os.path.join(output_dir, \"dataset_visualization.png\")\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Dataset visualization saved to {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during dataset visualization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53cd50",
   "metadata": {},
   "source": [
    "## Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fbe277",
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Keep ConciseProgressCallback and lr_step_decay exactly as they were\n",
    "\n",
    "class ConciseProgressCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Logs progress concisely and performs garbage collection.\"\"\"\n",
    "    def __init__(self, log_frequency=1):\n",
    "        super().__init__()\n",
    "        self.log_frequency = log_frequency\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "        if (epoch + 1) % self.log_frequency == 0:\n",
    "             print(f\"\\n--- Epoch {epoch + 1}/{self.params['epochs']} ---\")\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        if (epoch + 1) % self.log_frequency == 0:\n",
    "            metrics_str = \" - \".join([f\"{k}: {v:.4f}\" for k, v in logs.items()])\n",
    "            print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f}s - {metrics_str}\")\n",
    "            try: # Monitor Synergy weights\n",
    "                synergy_layer = self.model.get_layer('synergy')\n",
    "                alpha_val = synergy_layer.alpha.numpy()\n",
    "                beta_val = synergy_layer.beta.numpy()\n",
    "                print(f\"    Synergy weights - alpha: {alpha_val:.4f}, beta: {beta_val:.4f}\")\n",
    "            except Exception as e: pass # Layer might not exist\n",
    "        gc.collect() # Force garbage collection\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        total_time = time.time() - self.start_time\n",
    "        print(f\"\\n--- Training Finished ---\")\n",
    "        print(f\"Total training time: {total_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Learning Rate Scheduler Function\n",
    "def lr_step_decay(epoch, lr):\n",
    "    \"\"\"Applies step decay to the learning rate.\"\"\"\n",
    "    initial_lr = LEARNING_RATE\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10\n",
    "    new_lr = initial_lr * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "    final_lr = max(new_lr, 1e-7) # Minimum LR\n",
    "    return final_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3393ac73",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_distributed(\n",
    "    dataset_func=prepare_brats_data_gpu,\n",
    "    model_func=AS_Net,\n",
    "    strategy=strategy, # Pass the distribution strategy\n",
    "    checkpoint_path=CHECKPOINT_PATH, # Use variant-specific path\n",
    "    checkpoint_best_path=CHECKPOINT_BEST_PATH, # Use variant-specific path\n",
    "    output_dir=OUTPUT_DIR, # Use variant-specific path\n",
    "    img_height=IMG_HEIGHT,\n",
    "    img_width=IMG_WIDTH,\n",
    "    input_channels=INPUT_CHANNELS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    combined_loss_weights=COMBINED_LOSS_WEIGHTS,\n",
    "    metrics=['binary_accuracy',\n",
    "             DiceCoefficient(name='dice_coef'),\n",
    "             IoU(name='iou'),\n",
    "             tf.keras.metrics.Precision(thresholds=THRESHOLD, name=\"precision\"),\n",
    "             tf.keras.metrics.Recall(thresholds=THRESHOLD, name=\"recall\")],\n",
    "    efficientnet_variant=EFFICIENTNET_VARIANT # Pass variant to model function\n",
    "):\n",
    "    \"\"\"Trains the AS-Net EfficientNetV2 model using tf.distribute.Strategy.\"\"\"\n",
    "\n",
    "    global_batch_size = batch_size * strategy.num_replicas_in_sync\n",
    "    print(f\"--- Starting Training ({VARIANT_SUFFIX}) ---\") # Include variant\n",
    "    print(f\"Number of replicas: {strategy.num_replicas_in_sync}\")\n",
    "    print(f\"Global batch size: {global_batch_size}\")\n",
    "    print(f\"Target Image Size: {img_height}x{img_width}\")\n",
    "    print(f\"Epochs: {num_epochs}\")\n",
    "    print(f\"Initial Learning Rate: {learning_rate}\")\n",
    "    print(f\"Loss configuration: {combined_loss_weights}\")\n",
    "\n",
    "    # 1. Create Datasets within Strategy Scope\n",
    "    print(\"Preparing datasets...\")\n",
    "    # Pass global_batch_size and correct target size\n",
    "    train_dataset, val_dataset = dataset_func(\n",
    "        batch_size=global_batch_size,\n",
    "        target_size=(img_height, img_width)\n",
    "    )\n",
    "    print(\"Datasets prepared.\")\n",
    "\n",
    "    # Optionally visualize dataset samples before training\n",
    "    visualize_dataset_samples(train_dataset, num_samples=3, output_dir=output_dir)\n",
    "\n",
    "    # 2. Build and Compile Model within Strategy Scope\n",
    "    with strategy.scope():\n",
    "        print(\"Building model...\")\n",
    "        # Pass variant and input size to the model function\n",
    "        model = model_func(\n",
    "            input_size=(img_height, img_width, input_channels),\n",
    "            variant=efficientnet_variant\n",
    "        )\n",
    "        print(\"Model built.\")\n",
    "\n",
    "        print(\"Compiling model...\")\n",
    "        print(\"Using Combined WBCE + Dice Loss\")\n",
    "        loss_instance = CombinedLoss(\n",
    "             bce_weight=combined_loss_weights['bce_weight'],\n",
    "             dice_weight=combined_loss_weights['dice_weight'],\n",
    "             class_weight=combined_loss_weights['class_weight']\n",
    "        )\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        if mixed_precision.global_policy().name == 'mixed_float16':\n",
    "             optimizer = mixed_precision.LossScaleOptimizer(optimizer)\n",
    "             print(\"Loss scaling applied for mixed precision.\")\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss_instance,\n",
    "            metrics=metrics\n",
    "        )\n",
    "        print(\"Model compiled.\")\n",
    "        model.summary(line_length=120)\n",
    "\n",
    "    # Check for checkpoint resume\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(os.path.dirname(checkpoint_path))\n",
    "    initial_epoch = 0\n",
    "    if latest_checkpoint:\n",
    "        print(f\"Resuming training from checkpoint: {latest_checkpoint}\")\n",
    "        try:\n",
    "            with strategy.scope(): # Load weights within scope\n",
    "                model.load_weights(latest_checkpoint).expect_partial()\n",
    "            # Simple epoch extraction (adjust regex/split if needed)\n",
    "            try:\n",
    "                epoch_str = latest_checkpoint.split('_')[-1].split('.')[0] # Try common patterns\n",
    "                if not epoch_str.isdigit(): epoch_str = latest_checkpoint.split('.')[-2].split('-')[-1] # Another pattern\n",
    "                if epoch_str.isdigit(): initial_epoch = int(epoch_str)\n",
    "                print(f\"Successfully loaded weights. Starting from epoch {initial_epoch + 1}\")\n",
    "            except:\n",
    "                 print(\"Warning: Could not determine epoch from checkpoint name. Starting from epoch 0.\")\n",
    "                 initial_epoch = 0\n",
    "        except Exception as load_err:\n",
    "            print(f\"Error loading weights: {load_err}. Starting from scratch.\")\n",
    "            initial_epoch = 0\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting training from scratch.\")\n",
    "\n",
    "    # 3. Define Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_best_path, save_weights_only=True, monitor='val_dice_coef',\n",
    "            mode='max', save_best_only=True, verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_path, save_weights_only=True, save_freq='epoch', verbose=0\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=15, restore_best_weights=True, verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.LearningRateScheduler(lr_step_decay, verbose=0),\n",
    "        ConciseProgressCallback(log_frequency=1),\n",
    "        # tf.keras.callbacks.TensorBoard(log_dir=os.path.join(output_dir, 'logs'), histogram_freq=1) # Optional\n",
    "    ]\n",
    "\n",
    "    # 4. Train the Model\n",
    "    epochs_to_run = num_epochs - initial_epoch\n",
    "    print(f\"Starting training loop for {epochs_to_run} epochs (Total planned: {num_epochs})...\")\n",
    "    if epochs_to_run <= 0:\n",
    "        print(\"Training already completed based on initial_epoch. Skipping fit.\")\n",
    "        # If training completed, maybe load history? For now, just return None.\n",
    "        # Try to load history if it exists\n",
    "        hist_csv_file = os.path.join(output_dir, 'training_history.csv')\n",
    "        if os.path.exists(hist_csv_file):\n",
    "            print(f\"Loading existing training history from {hist_csv_file}\")\n",
    "            history_df = pd.read_csv(hist_csv_file)\n",
    "            # Convert DataFrame back to a history-like dictionary object if needed\n",
    "            # For simplicity, we can skip plotting if training didn't run this time\n",
    "            history = None # Indicate no new history generated\n",
    "        else:\n",
    "             history = None\n",
    "    else:\n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=num_epochs,\n",
    "            initial_epoch=initial_epoch,\n",
    "            callbacks=callbacks,\n",
    "            verbose=0 # Use custom callback\n",
    "        )\n",
    "\n",
    "    # 5. Save Training History (only if training ran)\n",
    "    if history and history.history:\n",
    "        try:\n",
    "            hist_df = pd.DataFrame(history.history)\n",
    "            hist_csv_file = os.path.join(output_dir, 'training_history.csv')\n",
    "            hist_df.to_csv(hist_csv_file, index=False)\n",
    "            print(f\"Training history saved to {hist_csv_file}\")\n",
    "            plot_training_history(history, output_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving/plotting training history: {e}\")\n",
    "    elif history is None and epochs_to_run <=0:\n",
    "        print(\"Skipping history saving/plotting as training was already complete.\")\n",
    "    else:\n",
    "        print(\"Warning: No training history generated or history object is empty.\")\n",
    "\n",
    "\n",
    "    # Cleanup\n",
    "    print(f\"Cleaning up resources after training ({VARIANT_SUFFIX})...\")\n",
    "    del train_dataset, val_dataset\n",
    "    gc.collect()\n",
    "    # Keep model instance for potential evaluation return\n",
    "    print(\"Cleaned up datasets.\")\n",
    "\n",
    "    return history, model\n",
    "# -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17edaf97",
   "metadata": {},
   "source": [
    "## Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad2d817",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_training_history(history, output_dir=OUTPUT_DIR):\n",
    "    \"\"\"Plots training & validation loss and metrics and saves the plot.\"\"\"\n",
    "    # This function remains the same as in the MobileNetV3 version\n",
    "    print(\"--- Plotting Training History ---\")\n",
    "    try:\n",
    "        history_dict = history.history\n",
    "        if not history_dict:\n",
    "             print(\"History object is empty. Skipping plotting.\")\n",
    "             return\n",
    "\n",
    "        epochs = range(1, len(history_dict['loss']) + 1)\n",
    "        metrics_to_plot = {'loss': 'Loss'}\n",
    "        for key in history_dict.keys():\n",
    "            if key.startswith('val_'): continue\n",
    "            if key == 'loss': continue\n",
    "            if key == 'dice_coef': metrics_to_plot['dice_coef'] = 'Dice Coef'\n",
    "            elif key == 'iou': metrics_to_plot['iou'] = 'IoU'\n",
    "            elif key == 'binary_accuracy': metrics_to_plot['binary_accuracy'] = 'Accuracy'\n",
    "            elif key == 'precision': metrics_to_plot['precision'] = 'Precision'\n",
    "            elif key == 'recall': metrics_to_plot['recall'] = 'Recall'\n",
    "            elif f'val_{key}' in history_dict:\n",
    "                 metrics_to_plot[key] = key.replace('_', ' ').title()\n",
    "\n",
    "        num_plots = len(metrics_to_plot)\n",
    "        if num_plots <= 1:\n",
    "            print(\"Warning: Only 'loss' metric found. Plotting loss only.\")\n",
    "            if num_plots == 0: return\n",
    "\n",
    "        plt.figure(figsize=(max(12, 6 * num_plots), 5))\n",
    "\n",
    "        plot_index = 1\n",
    "        for metric, title in metrics_to_plot.items():\n",
    "            plt.subplot(1, num_plots, plot_index)\n",
    "            val_metric = f'val_{metric}'\n",
    "            if metric in history_dict:\n",
    "                plt.plot(epochs, history_dict[metric], 'bo-', label=f'Training {title}')\n",
    "            if val_metric in history_dict:\n",
    "                plt.plot(epochs, history_dict[val_metric], 'ro-', label=f'Validation {title}')\n",
    "            plt.title(f'{title}')\n",
    "            plt.xlabel('Epoch')\n",
    "            if metric != 'loss' and val_metric in history_dict: plt.legend()\n",
    "            elif metric in history_dict and not val_metric in history_dict: plt.legend() # Show training legend if val missing\n",
    "            if metric != 'loss': plt.ylim([0, 1.05])\n",
    "            plt.grid(True)\n",
    "            plot_index += 1\n",
    "\n",
    "        plt.suptitle(f'AS-Net {EFFICIENTNET_VARIANT} Training History', fontsize=14)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        save_path = os.path.join(output_dir, \"training_history_plots.png\")\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Training history plots saved to {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting training history: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e3e47d",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df265b23",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model=None, # Pass the trained model or load from checkpoint\n",
    "    checkpoint_best_path=CHECKPOINT_BEST_PATH, # Use variant path\n",
    "    checkpoint_path = CHECKPOINT_PATH, # Last epoch path\n",
    "    output_folder=OUTPUT_DIR, # Use variant path\n",
    "    img_height=IMG_HEIGHT,\n",
    "    img_width=IMG_WIDTH,\n",
    "    input_channels=INPUT_CHANNELS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    dataset_func=prepare_brats_data_gpu,\n",
    "    threshold=THRESHOLD,\n",
    "    num_examples_to_save=5,\n",
    "    loss_config=COMBINED_LOSS_WEIGHTS,\n",
    "    efficientnet_variant=EFFICIENTNET_VARIANT # Pass variant\n",
    "):\n",
    "    \"\"\"Evaluates the trained AS-Net EfficientNetV2 model.\"\"\"\n",
    "    print(f\"\\n--- Starting Model Evaluation ({VARIANT_SUFFIX}) ---\")\n",
    "    evaluation_results = None\n",
    "\n",
    "    try:\n",
    "        # 1. Load Validation Data\n",
    "        global_eval_batch_size = batch_size * strategy.num_replicas_in_sync\n",
    "        print(f\"Loading validation data with batch size: {global_eval_batch_size}...\")\n",
    "        _, val_dataset = dataset_func(\n",
    "            batch_size=global_eval_batch_size,\n",
    "            target_size=(img_height, img_width), # Use correct size\n",
    "            validation_split=0.2\n",
    "        )\n",
    "        print(\"Validation dataset loaded.\")\n",
    "\n",
    "        # 2. Load or Use Provided Model\n",
    "        model_eval = None\n",
    "        if model is None:\n",
    "             print(f\"Loading model weights from best checkpoint: {checkpoint_best_path}\")\n",
    "             checkpoint_to_load = None\n",
    "             if os.path.exists(checkpoint_best_path):\n",
    "                 checkpoint_to_load = checkpoint_best_path\n",
    "             else:\n",
    "                 print(f\"Warning: Best checkpoint not found at {checkpoint_best_path}.\")\n",
    "                 last_checkpoint = tf.train.latest_checkpoint(os.path.dirname(checkpoint_path))\n",
    "                 # Check if index file exists for the checkpoint\n",
    "                 if last_checkpoint and os.path.exists(last_checkpoint + \".index\"):\n",
    "                      print(f\"Attempting to load last epoch checkpoint: {last_checkpoint}\")\n",
    "                      checkpoint_to_load = last_checkpoint\n",
    "                 else:\n",
    "                     print(f\"Error: No suitable checkpoint found in {os.path.dirname(checkpoint_path)}. Cannot evaluate.\")\n",
    "                     return None\n",
    "\n",
    "             print(\"Rebuilding model architecture for evaluation...\")\n",
    "             with strategy.scope():\n",
    "                 model_eval = AS_Net(\n",
    "                     input_size=(img_height, img_width, input_channels),\n",
    "                     variant=efficientnet_variant # Pass variant\n",
    "                 )\n",
    "                 print(\"Compiling evaluation model...\")\n",
    "                 loss_instance = CombinedLoss(**loss_config)\n",
    "                 optimizer = tf.keras.optimizers.Adam()\n",
    "                 if mixed_precision.global_policy().name == 'mixed_float16':\n",
    "                    optimizer = mixed_precision.LossScaleOptimizer(optimizer)\n",
    "\n",
    "                 model_eval.compile(\n",
    "                      optimizer=optimizer,\n",
    "                      loss=loss_instance,\n",
    "                      metrics=['binary_accuracy',\n",
    "                               DiceCoefficient(name='dice_coef'),\n",
    "                               IoU(name='iou'),\n",
    "                               tf.keras.metrics.Precision(thresholds=threshold, name=\"precision\"),\n",
    "                               tf.keras.metrics.Recall(thresholds=threshold, name=\"recall\")]\n",
    "                 )\n",
    "                 print(f\"Loading weights from {checkpoint_to_load}...\")\n",
    "                 load_status = model_eval.load_weights(checkpoint_to_load)\n",
    "                 load_status.expect_partial() # Allow optimizer state mismatch\n",
    "                 print(f\"Successfully loaded weights into new model instance.\")\n",
    "        else:\n",
    "            print(\"Using provided trained model instance for evaluation.\")\n",
    "            model_eval = model\n",
    "            if not model_eval.optimizer: # Compile if needed\n",
    "                 print(\"Compiling the provided model for evaluation...\")\n",
    "                 with strategy.scope():\n",
    "                      loss_instance = CombinedLoss(**loss_config)\n",
    "                      optimizer = tf.keras.optimizers.Adam()\n",
    "                      if mixed_precision.global_policy().name == 'mixed_float16': optimizer = mixed_precision.LossScaleOptimizer(optimizer)\n",
    "                      model_eval.compile(\n",
    "                           optimizer=optimizer, loss=loss_instance,\n",
    "                           metrics=['binary_accuracy', DiceCoefficient(name='dice_coef'), IoU(name='iou'),\n",
    "                                    tf.keras.metrics.Precision(thresholds=threshold, name=\"precision\"),\n",
    "                                    tf.keras.metrics.Recall(thresholds=threshold, name=\"recall\")]\n",
    "                      )\n",
    "                      print(\"Provided model compiled.\")\n",
    "\n",
    "        # 3. Evaluate\n",
    "        print(\"Evaluating model on validation set...\")\n",
    "        evaluation_results = model_eval.evaluate(val_dataset, verbose=1, return_dict=True)\n",
    "        print(\"\\nKeras Evaluation Results:\")\n",
    "        for name, value in evaluation_results.items():\n",
    "            print(f\"- {name}: {value:.4f}\")\n",
    "\n",
    "        # 4. Calculate F1 Score\n",
    "        precision_val = evaluation_results.get('precision', 0.0)\n",
    "        recall_val = evaluation_results.get('recall', 0.0)\n",
    "        f1_val = 0.0\n",
    "        if (precision_val + recall_val) > 1e-7:\n",
    "            f1_val = 2 * (precision_val * recall_val) / (precision_val + recall_val)\n",
    "        evaluation_results['f1_score'] = f1_val\n",
    "        print(f\"- f1_score: {f1_val:.4f} (calculated)\")\n",
    "\n",
    "        # 5. Save Performance Metrics\n",
    "        try:\n",
    "            perf_file_path = os.path.join(output_folder, \"performances.txt\")\n",
    "            with open(perf_file_path, \"w\") as file_perf:\n",
    "                file_perf.write(f\"Evaluation Metrics ({VARIANT_SUFFIX}):\\n\")\n",
    "                file_perf.write(\"------------------------------------\\n\")\n",
    "                # Iterate through ordered keys if possible, or just the dict\n",
    "                metric_order = ['loss', 'binary_accuracy', 'dice_coef', 'iou', 'precision', 'recall', 'f1_score']\n",
    "                for name in metric_order:\n",
    "                     if name in evaluation_results:\n",
    "                          file_perf.write(f\"- {name.replace('_', ' ').title()}: {evaluation_results[name]:.4f}\\n\")\n",
    "                # Add any other metrics not in the predefined order\n",
    "                for name, value in evaluation_results.items():\n",
    "                     if name not in metric_order:\n",
    "                          file_perf.write(f\"- {name.replace('_', ' ').title()}: {value:.4f}\\n\")\n",
    "\n",
    "            print(f\"Evaluation results saved to {perf_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving performance metrics: {e}\")\n",
    "\n",
    "        # 6. Save Prediction Examples\n",
    "        print(\"\\nGenerating prediction examples...\")\n",
    "        save_prediction_examples(model_eval, val_dataset, output_folder, num_examples=num_examples_to_save, threshold=threshold)\n",
    "\n",
    "        print(f\"--- Evaluation Finished ({VARIANT_SUFFIX}) ---\")\n",
    "        return evaluation_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    finally:\n",
    "        print(\"Cleaning up resources after evaluation...\")\n",
    "        if 'val_dataset' in locals(): del val_dataset\n",
    "        # Don't delete model_eval if it's the same as the passed 'model'\n",
    "        if 'model_eval' in locals() and model_eval is not model: del model_eval\n",
    "        gc.collect()\n",
    "        print(\"Cleaned up evaluation resources.\")\n",
    "# -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6895ee34",
   "metadata": {},
   "source": [
    "## Save Prediction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf9dbba",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_prediction_examples(model, dataset, output_folder, num_examples=5, threshold=THRESHOLD):\n",
    "    \"\"\"Saves example predictions with inputs (Z-scored) and ground truth.\"\"\"\n",
    "    # This function remains largely the same, showing Z-scored input\n",
    "    print(f\"Saving {num_examples} prediction examples...\")\n",
    "    examples_dir = os.path.join(output_folder, \"examples\")\n",
    "    os.makedirs(examples_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        for images, masks in dataset.take(1): # Take one batch\n",
    "            print(f\"Generating predictions for {min(num_examples, images.shape[0])} examples...\")\n",
    "            predictions = model.predict(images) # Model handles its internal preprocessing\n",
    "            binary_predictions = tf.cast(predictions >= threshold, tf.float32).numpy()\n",
    "            predictions = predictions.numpy()\n",
    "            images_numpy = images.numpy() # Z-scored images from dataset\n",
    "            masks = masks.numpy()\n",
    "\n",
    "            print(\"Plotting and saving examples...\")\n",
    "            for j in range(min(num_examples, images_numpy.shape[0])):\n",
    "                plt.figure(figsize=(16, 4))\n",
    "                img_zscored = images_numpy[j]\n",
    "                # Scale Z-scored data approx to [0,1] for display\n",
    "                p_low, p_high = np.percentile(img_zscored.flatten(), [2, 98])\n",
    "                img_display = np.clip((img_zscored - p_low) / (p_high - p_low + 1e-8), 0, 1)\n",
    "                if np.isnan(img_display).any() or np.isinf(img_display).any(): img_display = np.nan_to_num(img_display)\n",
    "\n",
    "                # Plot 1: Input (Z-scored, Scaled)\n",
    "                plt.subplot(1, 4, 1)\n",
    "                plt.title(\"Input (Z-scored, Scaled)\")\n",
    "                plt.imshow(img_display)\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                # Plot 2: Ground Truth Mask\n",
    "                plt.subplot(1, 4, 2)\n",
    "                plt.title(\"Ground Truth Mask\")\n",
    "                plt.imshow(img_display, cmap='gray', alpha=0.6)\n",
    "                gt_mask_display = masks[j].squeeze()\n",
    "                if np.isnan(gt_mask_display).any() or np.isinf(gt_mask_display).any(): gt_mask_display = np.nan_to_num(gt_mask_display)\n",
    "                plt.imshow(gt_mask_display, cmap='viridis', alpha=0.5, vmin=0, vmax=1)\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                # Plot 3: Prediction Probabilities\n",
    "                plt.subplot(1, 4, 3)\n",
    "                plt.title(\"Prediction Probabilities\")\n",
    "                plt.imshow(img_display, cmap='gray', alpha=0.6)\n",
    "                pred_prob_display = predictions[j].squeeze()\n",
    "                if np.isnan(pred_prob_display).any() or np.isinf(pred_prob_display).any(): pred_prob_display = np.nan_to_num(pred_prob_display)\n",
    "                prob_map = plt.imshow(pred_prob_display, cmap='hot', alpha=0.5, vmin=0, vmax=1)\n",
    "                plt.axis(\"off\")\n",
    "                # plt.colorbar(prob_map, fraction=0.046, pad=0.04) # Optional\n",
    "\n",
    "                # Plot 4: Binary Prediction\n",
    "                plt.subplot(1, 4, 4)\n",
    "                plt.title(f\"Binary Prediction (t={threshold:.2f})\")\n",
    "                plt.imshow(img_display, cmap='gray', alpha=0.6)\n",
    "                binary_pred_display = binary_predictions[j].squeeze()\n",
    "                if np.isnan(binary_pred_display).any() or np.isinf(binary_pred_display).any(): binary_pred_display = np.nan_to_num(binary_pred_display)\n",
    "                plt.imshow(binary_pred_display, cmap='viridis', alpha=0.5, vmin=0, vmax=1)\n",
    "                plt.axis(\"off\")\n",
    "\n",
    "                plt.tight_layout(pad=0.5)\n",
    "                example_save_path = os.path.join(examples_dir, f\"prediction_example_{j+1}.png\")\n",
    "                plt.savefig(example_save_path, dpi=150, bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "            print(f\"Saved prediction examples to {examples_dir}\")\n",
    "            break # Only one batch\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving prediction examples: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12350329",
   "metadata": {},
   "source": [
    "## Completion Notification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dacd2c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_completion_notification(output_folder=OUTPUT_DIR, completion_file=COMPLETION_FILE, start_time=None):\n",
    "    \"\"\"Creates a text file summarizing the training run and results.\"\"\"\n",
    "    print(\"\\n--- Creating Completion Notification ---\")\n",
    "    import datetime\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    perf_file_path = os.path.join(output_folder, \"performances.txt\")\n",
    "\n",
    "    duration_str = \"Unknown (start time not recorded)\"\n",
    "    if start_time is not None:\n",
    "        duration_seconds = time.time() - start_time\n",
    "        hours = int(duration_seconds // 3600)\n",
    "        minutes = int((duration_seconds % 3600) // 60)\n",
    "        seconds = int(duration_seconds % 60)\n",
    "        duration_str = f\"{hours}h {minutes}m {seconds}s\"\n",
    "\n",
    "    try:\n",
    "        with open(completion_file, \"w\") as f:\n",
    "            f.write(f\"AS-Net ({VARIANT_SUFFIX}) Training Completed at: {timestamp}\\n\\n\")\n",
    "            f.write(\"Training Configuration:\\n\")\n",
    "            f.write(f\"- Model: AS-Net with {EFFICIENTNET_VARIANT} encoder\\n\") # Use variant name\n",
    "            f.write(f\"- Image dimensions: {IMG_HEIGHT}x{IMG_WIDTH}\\n\") # Use correct size\n",
    "            f.write(f\"- Input Channels: {INPUT_CHANNELS}\\n\")\n",
    "            f.write(f\"- Batch size (per replica): {BATCH_SIZE}\\n\")\n",
    "            f.write(f\"- Global Batch size: {BATCH_SIZE * strategy.num_replicas_in_sync}\\n\")\n",
    "            f.write(f\"- Epochs planned: {NUM_EPOCHS}\\n\")\n",
    "            f.write(f\"- Initial Learning rate: {LEARNING_RATE}\\n\")\n",
    "            f.write(f\"- Mixed Precision Policy: {mixed_precision.global_policy().name}\\n\")\n",
    "            f.write(f\"- Loss Config: {COMBINED_LOSS_WEIGHTS}\\n\")\n",
    "            f.write(f\"- Total Duration: {duration_str}\\n\\n\")\n",
    "\n",
    "            f.write(\"Checkpoint and output locations:\\n\")\n",
    "            f.write(f\"- Checkpoint directory: {CHECKPOINT_DIR}\\n\") # Use variant path\n",
    "            f.write(f\"- Best model weights: {CHECKPOINT_BEST_PATH}\\n\") # Use variant path\n",
    "            f.write(f\"- Output directory: {output_folder}\\n\") # Use variant path\n",
    "\n",
    "            f.write(\"\\n--- Final Performance Metrics ---\\n\")\n",
    "            if os.path.exists(perf_file_path):\n",
    "                try:\n",
    "                    with open(perf_file_path, \"r\") as perf_file:\n",
    "                        f.write(perf_file.read())\n",
    "                except Exception as read_err:\n",
    "                    f.write(f\"Note: Error reading performance file ({perf_file_path}): {read_err}\\n\")\n",
    "            else:\n",
    "                f.write(f\"Note: Performance file not found ({perf_file_path}). Evaluation failed or not run.\\n\")\n",
    "\n",
    "        print(f\"Completion notification saved to: {completion_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating completion notification file: {e}\")\n",
    "# -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6150cdf1",
   "metadata": {},
   "source": [
    "## Execute Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7928a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "script_start_time = time.time()\n",
    "\n",
    "# Step 1: Train the model (or skip if complete)\n",
    "model = None\n",
    "history = None\n",
    "\n",
    "if os.path.exists(COMPLETION_FILE):\n",
    "     print(f\"Completion file '{COMPLETION_FILE}' found. Skipping training.\")\n",
    "else:\n",
    "     print(f\"Completion file '{COMPLETION_FILE}' not found. Starting training process...\")\n",
    "     # Pass EFFICIENTNET_VARIANT to the training function\n",
    "     history, model = train_model_distributed(\n",
    "         efficientnet_variant=EFFICIENTNET_VARIANT,\n",
    "         img_height=IMG_HEIGHT, # Pass correct dimensions\n",
    "         img_width=IMG_WIDTH\n",
    "     )\n",
    "\n",
    "# Step 2: Evaluate the model\n",
    "# Pass EFFICIENTNET_VARIANT and dimensions to evaluation\n",
    "evaluation_results = evaluate_model(\n",
    "    model=model, # Pass trained model if available, otherwise it loads from checkpoint\n",
    "    efficientnet_variant=EFFICIENTNET_VARIANT,\n",
    "    img_height=IMG_HEIGHT, # Pass correct dimensions\n",
    "    img_width=IMG_WIDTH\n",
    ")\n",
    "\n",
    "# Step 3: Create completion notification\n",
    "create_completion_notification(start_time=script_start_time)\n",
    "\n",
    "# Final cleanup\n",
    "print(\"\\n--- Final Script Cleanup ---\")\n",
    "# Clear potentially large objects\n",
    "if 'model' in locals() and model is not None: del model\n",
    "if 'train_dataset' in locals(): del train_dataset\n",
    "if 'val_dataset' in locals(): del val_dataset\n",
    "if 'evaluation_results' in locals(): del evaluation_results\n",
    "if 'history' in locals(): del history\n",
    "gc.collect()\n",
    "backend.clear_session()\n",
    "print(f\"Script execution completed successfully for {VARIANT_SUFFIX}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
